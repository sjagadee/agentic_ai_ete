{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3185b8c0",
   "metadata": {},
   "source": [
    "### Agentic RAG\n",
    "\n",
    "Here is the workflow for current implementation\n",
    "\n",
    "- Use two vector db's \n",
    "    - first db will have LangGraph related blogs\n",
    "    - second db will have LangChain related blogs\n",
    "- An Agent (Agent 1) will decide based on the user query to choose first db or second db\n",
    "- Once the related documents are fetched, we check if the documents have correct information\n",
    "    - If the information is selected document context is not appropriate\n",
    "    - Rewrite (Agent 2) the query and send the updated query back to the agent\n",
    "- If the info is correct - pass it as contect to the Generator (Agent 3) and finally output the response\n",
    "- If an unrelated query is asked agent (Agent 1) will directly reject and appropriate answer will be sent as output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af56c486",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/srinivas/Documents/Others/My_projects/Python/DSAIML/agenticai/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.embeddings import init_embeddings\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AnyMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "from langsmith import Client\n",
    "\n",
    "from typing import Annotated, TypedDict, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1275f8",
   "metadata": {},
   "source": [
    "### Model and Embedding initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "002fe73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the project name for tracing\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Agentic RAG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f0fc875",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_chat_model(\n",
    "    model=\"qwen/qwen3-32b\",  # or any other Groq model\n",
    "    model_provider=\"groq\",\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "embeddings = init_embeddings(\"openai:text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9769c2",
   "metadata": {},
   "source": [
    "### Load, extract and flatten documents from Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00e84ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/quickstart', 'title': 'Quickstart - Docs by LangChain', 'language': 'en'}, page_content='Quickstart - Docs by LangChainSkip to main contentüöÄ Share how you\\'re building agents for a chance to win LangChain swag!Docs by LangChain home pageLangChain + LangGraphSearch...‚åòKAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationGet startedQuickstartLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureTestLangSmith StudioAgent Chat UILangSmith DeploymentLangSmith ObservabilityLangGraph APIsGraph APIFunctional APIRuntimeGet startedQuickstartCopy pageCopy pageThis quickstart demonstrates how to build a calculator agent using the LangGraph Graph API or the Functional API.\\n\\nUse the Graph API if you prefer to define your agent as a graph of nodes and edges.\\nUse the Functional API if you prefer to define your agent as a single function.\\n\\nFor conceptual information, see Graph API overview and Functional API overview.\\nFor this example, you will need to set up a Claude (Anthropic) account and get an API key. Then, set the ANTHROPIC_API_KEY environment variable in your terminal.\\n Use the Graph API Use the Functional API\\u200b1. Define tools and modelIn this example, we‚Äôll use the Claude Sonnet 4.5 model and define tools for addition, multiplication, and division.Copyfrom langchain.tools import tool\\nfrom langchain.chat_models import init_chat_model\\n\\n\\nmodel = init_chat_model(\\n    \"claude-sonnet-4-5-20250929\",\\n    temperature=0\\n)\\n\\n\\n# Define tools\\n@tool\\ndef multiply(a: int, b: int) -> int:\\n    \"\"\"Multiply `a` and `b`.\\n\\n    Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a * b\\n\\n\\n@tool\\ndef add(a: int, b: int) -> int:\\n    \"\"\"Adds `a` and `b`.\\n\\n    Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a + b\\n\\n\\n@tool\\ndef divide(a: int, b: int) -> float:\\n    \"\"\"Divide `a` and `b`.\\n\\n    Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a / b\\n\\n\\n# Augment the LLM with tools\\ntools = [add, multiply, divide]\\ntools_by_name = {tool.name: tool for tool in tools}\\nmodel_with_tools = model.bind_tools(tools)\\n\\u200b2. Define stateThe graph‚Äôs state is used to store the messages and the number of LLM calls.State in LangGraph persists throughout the agent‚Äôs execution.The Annotated type with operator.add ensures that new messages are appended to the existing list rather than replacing it.Copyfrom langchain.messages import AnyMessage\\nfrom typing_extensions import TypedDict, Annotated\\nimport operator\\n\\n\\nclass MessagesState(TypedDict):\\n    messages: Annotated[list[AnyMessage], operator.add]\\n    llm_calls: int\\n\\u200b3. Define model nodeThe model node is used to call the LLM and decide whether to call a tool or not.Copyfrom langchain.messages import SystemMessage\\n\\n\\ndef llm_call(state: dict):\\n    \"\"\"LLM decides whether to call a tool or not\"\"\"\\n\\n    return {\\n        \"messages\": [\\n            model_with_tools.invoke(\\n                [\\n                    SystemMessage(\\n                        content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\\n                    )\\n                ]\\n                + state[\"messages\"]\\n            )\\n        ],\\n        \"llm_calls\": state.get(\\'llm_calls\\', 0) + 1\\n    }\\n\\u200b4. Define tool nodeThe tool node is used to call the tools and return the results.Copyfrom langchain.messages import ToolMessage\\n\\n\\ndef tool_node(state: dict):\\n    \"\"\"Performs the tool call\"\"\"\\n\\n    result = []\\n    for tool_call in state[\"messages\"][-1].tool_calls:\\n        tool = tools_by_name[tool_call[\"name\"]]\\n        observation = tool.invoke(tool_call[\"args\"])\\n        result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\\n    return {\"messages\": result}\\n\\u200b5. Define end logicThe conditional edge function is used to route to the tool node or end based upon whether the LLM made a tool call.Copyfrom typing import Literal\\nfrom langgraph.graph import StateGraph, START, END\\n\\n\\ndef should_continue(state: MessagesState) -> Literal[\"tool_node\", END]:\\n    \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\\n\\n    messages = state[\"messages\"]\\n    last_message = messages[-1]\\n\\n    # If the LLM makes a tool call, then perform an action\\n    if last_message.tool_calls:\\n        return \"tool_node\"\\n\\n    # Otherwise, we stop (reply to the user)\\n    return END\\n\\u200b6. Build and compile the agentThe agent is built using the StateGraph class and compiled using the compile method.Copy# Build workflow\\nagent_builder = StateGraph(MessagesState)\\n\\n# Add nodes\\nagent_builder.add_node(\"llm_call\", llm_call)\\nagent_builder.add_node(\"tool_node\", tool_node)\\n\\n# Add edges to connect nodes\\nagent_builder.add_edge(START, \"llm_call\")\\nagent_builder.add_conditional_edges(\\n    \"llm_call\",\\n    should_continue,\\n    [\"tool_node\", END]\\n)\\nagent_builder.add_edge(\"tool_node\", \"llm_call\")\\n\\n# Compile the agent\\nagent = agent_builder.compile()\\n\\n# Show the agent\\nfrom IPython.display import Image, display\\ndisplay(Image(agent.get_graph(xray=True).draw_mermaid_png()))\\n\\n# Invoke\\nfrom langchain.messages import HumanMessage\\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\\nmessages = agent.invoke({\"messages\": messages})\\nfor m in messages[\"messages\"]:\\n    m.pretty_print()\\nTo learn how to trace your agent with LangSmith, see the LangSmith documentation.Congratulations! You‚Äôve built your first agent using the LangGraph Graph API.Full code exampleCopy# Step 1: Define tools and model\\n\\nfrom langchain.tools import tool\\nfrom langchain.chat_models import init_chat_model\\n\\n\\nmodel = init_chat_model(\\n    \"claude-sonnet-4-5-20250929\",\\n    temperature=0\\n)\\n\\n\\n# Define tools\\n@tool\\ndef multiply(a: int, b: int) -> int:\\n    \"\"\"Multiply `a` and `b`.\\n\\n    Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a * b\\n\\n\\n@tool\\ndef add(a: int, b: int) -> int:\\n    \"\"\"Adds `a` and `b`.\\n\\n    Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a + b\\n\\n\\n@tool\\ndef divide(a: int, b: int) -> float:\\n    \"\"\"Divide `a` and `b`.\\n\\n    Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a / b\\n\\n\\n# Augment the LLM with tools\\ntools = [add, multiply, divide]\\ntools_by_name = {tool.name: tool for tool in tools}\\nmodel_with_tools = model.bind_tools(tools)\\n\\n# Step 2: Define state\\n\\nfrom langchain.messages import AnyMessage\\nfrom typing_extensions import TypedDict, Annotated\\nimport operator\\n\\n\\nclass MessagesState(TypedDict):\\n    messages: Annotated[list[AnyMessage], operator.add]\\n    llm_calls: int\\n\\n# Step 3: Define model node\\nfrom langchain.messages import SystemMessage\\n\\n\\ndef llm_call(state: dict):\\n    \"\"\"LLM decides whether to call a tool or not\"\"\"\\n\\n    return {\\n        \"messages\": [\\n            model_with_tools.invoke(\\n                [\\n                    SystemMessage(\\n                        content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\\n                    )\\n                ]\\n                + state[\"messages\"]\\n            )\\n        ],\\n        \"llm_calls\": state.get(\\'llm_calls\\', 0) + 1\\n    }\\n\\n\\n# Step 4: Define tool node\\n\\nfrom langchain.messages import ToolMessage\\n\\n\\ndef tool_node(state: dict):\\n    \"\"\"Performs the tool call\"\"\"\\n\\n    result = []\\n    for tool_call in state[\"messages\"][-1].tool_calls:\\n        tool = tools_by_name[tool_call[\"name\"]]\\n        observation = tool.invoke(tool_call[\"args\"])\\n        result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\\n    return {\"messages\": result}\\n\\n# Step 5: Define logic to determine whether to end\\n\\nfrom typing import Literal\\nfrom langgraph.graph import StateGraph, START, END\\n\\n\\n# Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call\\ndef should_continue(state: MessagesState) -> Literal[\"tool_node\", END]:\\n    \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\\n\\n    messages = state[\"messages\"]\\n    last_message = messages[-1]\\n\\n    # If the LLM makes a tool call, then perform an action\\n    if last_message.tool_calls:\\n        return \"tool_node\"\\n\\n    # Otherwise, we stop (reply to the user)\\n    return END\\n\\n# Step 6: Build agent\\n\\n# Build workflow\\nagent_builder = StateGraph(MessagesState)\\n\\n# Add nodes\\nagent_builder.add_node(\"llm_call\", llm_call)\\nagent_builder.add_node(\"tool_node\", tool_node)\\n\\n# Add edges to connect nodes\\nagent_builder.add_edge(START, \"llm_call\")\\nagent_builder.add_conditional_edges(\\n    \"llm_call\",\\n    should_continue,\\n    [\"tool_node\", END]\\n)\\nagent_builder.add_edge(\"tool_node\", \"llm_call\")\\n\\n# Compile the agent\\nagent = agent_builder.compile()\\n\\n\\nfrom IPython.display import Image, display\\n# Show the agent\\ndisplay(Image(agent.get_graph(xray=True).draw_mermaid_png()))\\n\\n# Invoke\\nfrom langchain.messages import HumanMessage\\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\\nmessages = agent.invoke({\"messages\": messages})\\nfor m in messages[\"messages\"]:\\n    m.pretty_print()\\n\\n\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoInstall LangGraphPreviousRun a local serverNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='Workflows and agents - Docs by LangChainSkip to main contentüöÄ Share how you\\'re building agents for a chance to win LangChain swag!Docs by LangChain home pageLangChain + LangGraphSearch...‚åòKAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationGet startedWorkflows and agentsLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureTestLangSmith StudioAgent Chat UILangSmith DeploymentLangSmith ObservabilityLangGraph APIsGraph APIFunctional APIRuntimeOn this pageSetupLLMs and augmentationsPrompt chainingParallelizationRoutingOrchestrator-workerCreating workers in LangGraphEvaluator-optimizerAgentsGet startedWorkflows and agentsCopy pageCopy pageThis guide reviews common workflow and agent patterns.\\n\\nWorkflows have predetermined code paths and are designed to operate in a certain order.\\nAgents are dynamic and define their own processes and tool usage.\\n\\n\\nLangGraph offers several benefits when building agents and workflows, including persistence, streaming, and support for debugging as well as deployment.\\n\\u200bSetup\\nTo build a workflow or agent, you can use any chat model that supports structured outputs and tool calling. The following example uses Anthropic:\\n\\nInstall dependencies:\\n\\nCopypip install langchain_core langchain-anthropic langgraph\\n\\n\\nInitialize the LLM:\\n\\nCopyimport os\\nimport getpass\\n\\nfrom langchain_anthropic import ChatAnthropic\\n\\ndef _set_env(var: str):\\n    if not os.environ.get(var):\\n        os.environ[var] = getpass.getpass(f\"{var}: \")\\n\\n\\n_set_env(\"ANTHROPIC_API_KEY\")\\n\\nllm = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\")\\n\\n\\u200bLLMs and augmentations\\nWorkflows and agentic systems are based on LLMs and the various augmentations you add to them. Tool calling, structured outputs, and short term memory are a few options for tailoring LLMs to your needs.\\n\\nCopy# Schema for structured output\\nfrom pydantic import BaseModel, Field\\n\\n\\nclass SearchQuery(BaseModel):\\n    search_query: str = Field(None, description=\"Query that is optimized web search.\")\\n    justification: str = Field(\\n        None, description=\"Why this query is relevant to the user\\'s request.\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nstructured_llm = llm.with_structured_output(SearchQuery)\\n\\n# Invoke the augmented LLM\\noutput = structured_llm.invoke(\"How does Calcium CT score relate to high cholesterol?\")\\n\\n# Define a tool\\ndef multiply(a: int, b: int) -> int:\\n    return a * b\\n\\n# Augment the LLM with tools\\nllm_with_tools = llm.bind_tools([multiply])\\n\\n# Invoke the LLM with input that triggers the tool call\\nmsg = llm_with_tools.invoke(\"What is 2 times 3?\")\\n\\n# Get the tool call\\nmsg.tool_calls\\n\\n\\u200bPrompt chaining\\nPrompt chaining is when each LLM call processes the output of the previous call. It‚Äôs often used for performing well-defined tasks that can be broken down into smaller, verifiable steps. Some examples include:\\n\\nTranslating documents into different languages\\nVerifying generated content for consistency\\n\\n\\nGraph APIFunctional APICopyfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\nfrom IPython.display import Image, display\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str\\n    joke: str\\n    improved_joke: str\\n    final_joke: str\\n\\n\\n# Nodes\\ndef generate_joke(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n\\n    msg = llm.invoke(f\"Write a short joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef check_punchline(state: State):\\n    \"\"\"Gate function to check if the joke has a punchline\"\"\"\\n\\n    # Simple check - does the joke contain \"?\" or \"!\"\\n    if \"?\" in state[\"joke\"] or \"!\" in state[\"joke\"]:\\n        return \"Pass\"\\n    return \"Fail\"\\n\\n\\ndef improve_joke(state: State):\\n    \"\"\"Second LLM call to improve the joke\"\"\"\\n\\n    msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {state[\\'joke\\']}\")\\n    return {\"improved_joke\": msg.content}\\n\\n\\ndef polish_joke(state: State):\\n    \"\"\"Third LLM call for final polish\"\"\"\\n    msg = llm.invoke(f\"Add a surprising twist to this joke: {state[\\'improved_joke\\']}\")\\n    return {\"final_joke\": msg.content}\\n\\n\\n# Build workflow\\nworkflow = StateGraph(State)\\n\\n# Add nodes\\nworkflow.add_node(\"generate_joke\", generate_joke)\\nworkflow.add_node(\"improve_joke\", improve_joke)\\nworkflow.add_node(\"polish_joke\", polish_joke)\\n\\n# Add edges to connect nodes\\nworkflow.add_edge(START, \"generate_joke\")\\nworkflow.add_conditional_edges(\\n    \"generate_joke\", check_punchline, {\"Fail\": \"improve_joke\", \"Pass\": END}\\n)\\nworkflow.add_edge(\"improve_joke\", \"polish_joke\")\\nworkflow.add_edge(\"polish_joke\", END)\\n\\n# Compile\\nchain = workflow.compile()\\n\\n# Show workflow\\ndisplay(Image(chain.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = chain.invoke({\"topic\": \"cats\"})\\nprint(\"Initial joke:\")\\nprint(state[\"joke\"])\\nprint(\"\\\\n--- --- ---\\\\n\")\\nif \"improved_joke\" in state:\\n    print(\"Improved joke:\")\\n    print(state[\"improved_joke\"])\\n    print(\"\\\\n--- --- ---\\\\n\")\\n\\n    print(\"Final joke:\")\\n    print(state[\"final_joke\"])\\nelse:\\n    print(\"Final joke:\")\\n    print(state[\"joke\"])\\n\\n\\u200bParallelization\\nWith parallelization, LLMs work simultaneously on a task. This is either done by running multiple independent subtasks at the same time, or running the same task multiple times to check for different outputs. Parallelization is commonly used to:\\n\\nSplit up subtasks and run them in parallel, which increases speed\\nRun tasks multiple times to check for different outputs, which increases confidence\\n\\nSome examples include:\\n\\nRunning one subtask that processes a document for keywords, and a second subtask to check for formatting errors\\nRunning a task multiple times that scores a document for accuracy based on different criteria, like the number of citations, the number of sources used, and the quality of the sources\\n\\n\\nGraph APIFunctional APICopy# Graph state\\nclass State(TypedDict):\\n    topic: str\\n    joke: str\\n    story: str\\n    poem: str\\n    combined_output: str\\n\\n\\n# Nodes\\ndef call_llm_1(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n\\n    msg = llm.invoke(f\"Write a joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef call_llm_2(state: State):\\n    \"\"\"Second LLM call to generate story\"\"\"\\n\\n    msg = llm.invoke(f\"Write a story about {state[\\'topic\\']}\")\\n    return {\"story\": msg.content}\\n\\n\\ndef call_llm_3(state: State):\\n    \"\"\"Third LLM call to generate poem\"\"\"\\n\\n    msg = llm.invoke(f\"Write a poem about {state[\\'topic\\']}\")\\n    return {\"poem\": msg.content}\\n\\n\\ndef aggregator(state: State):\\n    \"\"\"Combine the joke and story into a single output\"\"\"\\n\\n    combined = f\"Here\\'s a story, joke, and poem about {state[\\'topic\\']}!\\\\n\\\\n\"\\n    combined += f\"STORY:\\\\n{state[\\'story\\']}\\\\n\\\\n\"\\n    combined += f\"JOKE:\\\\n{state[\\'joke\\']}\\\\n\\\\n\"\\n    combined += f\"POEM:\\\\n{state[\\'poem\\']}\"\\n    return {\"combined_output\": combined}\\n\\n\\n# Build workflow\\nparallel_builder = StateGraph(State)\\n\\n# Add nodes\\nparallel_builder.add_node(\"call_llm_1\", call_llm_1)\\nparallel_builder.add_node(\"call_llm_2\", call_llm_2)\\nparallel_builder.add_node(\"call_llm_3\", call_llm_3)\\nparallel_builder.add_node(\"aggregator\", aggregator)\\n\\n# Add edges to connect nodes\\nparallel_builder.add_edge(START, \"call_llm_1\")\\nparallel_builder.add_edge(START, \"call_llm_2\")\\nparallel_builder.add_edge(START, \"call_llm_3\")\\nparallel_builder.add_edge(\"call_llm_1\", \"aggregator\")\\nparallel_builder.add_edge(\"call_llm_2\", \"aggregator\")\\nparallel_builder.add_edge(\"call_llm_3\", \"aggregator\")\\nparallel_builder.add_edge(\"aggregator\", END)\\nparallel_workflow = parallel_builder.compile()\\n\\n# Show workflow\\ndisplay(Image(parallel_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = parallel_workflow.invoke({\"topic\": \"cats\"})\\nprint(state[\"combined_output\"])\\n\\n\\u200bRouting\\nRouting workflows process inputs and then directs them to context-specific tasks. This allows you to define specialized flows for complex tasks. For example, a workflow built to answer product related questions might process the type of question first, and then route the request to specific processes for pricing, refunds, returns, etc.\\n\\nGraph APIFunctional APICopyfrom typing_extensions import Literal\\nfrom langchain.messages import HumanMessage, SystemMessage\\n\\n\\n# Schema for structured output to use as routing logic\\nclass Route(BaseModel):\\n    step: Literal[\"poem\", \"story\", \"joke\"] = Field(\\n        None, description=\"The next step in the routing process\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nrouter = llm.with_structured_output(Route)\\n\\n\\n# State\\nclass State(TypedDict):\\n    input: str\\n    decision: str\\n    output: str\\n\\n\\n# Nodes\\ndef llm_call_1(state: State):\\n    \"\"\"Write a story\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_2(state: State):\\n    \"\"\"Write a joke\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_3(state: State):\\n    \"\"\"Write a poem\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_router(state: State):\\n    \"\"\"Route the input to the appropriate node\"\"\"\\n\\n    # Run the augmented LLM with structured output to serve as routing logic\\n    decision = router.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Route the input to story, joke, or poem based on the user\\'s request.\"\\n            ),\\n            HumanMessage(content=state[\"input\"]),\\n        ]\\n    )\\n\\n    return {\"decision\": decision.step}\\n\\n\\n# Conditional edge function to route to the appropriate node\\ndef route_decision(state: State):\\n    # Return the node name you want to visit next\\n    if state[\"decision\"] == \"story\":\\n        return \"llm_call_1\"\\n    elif state[\"decision\"] == \"joke\":\\n        return \"llm_call_2\"\\n    elif state[\"decision\"] == \"poem\":\\n        return \"llm_call_3\"\\n\\n\\n# Build workflow\\nrouter_builder = StateGraph(State)\\n\\n# Add nodes\\nrouter_builder.add_node(\"llm_call_1\", llm_call_1)\\nrouter_builder.add_node(\"llm_call_2\", llm_call_2)\\nrouter_builder.add_node(\"llm_call_3\", llm_call_3)\\nrouter_builder.add_node(\"llm_call_router\", llm_call_router)\\n\\n# Add edges to connect nodes\\nrouter_builder.add_edge(START, \"llm_call_router\")\\nrouter_builder.add_conditional_edges(\\n    \"llm_call_router\",\\n    route_decision,\\n    {  # Name returned by route_decision : Name of next node to visit\\n        \"llm_call_1\": \"llm_call_1\",\\n        \"llm_call_2\": \"llm_call_2\",\\n        \"llm_call_3\": \"llm_call_3\",\\n    },\\n)\\nrouter_builder.add_edge(\"llm_call_1\", END)\\nrouter_builder.add_edge(\"llm_call_2\", END)\\nrouter_builder.add_edge(\"llm_call_3\", END)\\n\\n# Compile workflow\\nrouter_workflow = router_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(router_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = router_workflow.invoke({\"input\": \"Write me a joke about cats\"})\\nprint(state[\"output\"])\\n\\n\\u200bOrchestrator-worker\\nIn an orchestrator-worker configuration, the orchestrator:\\n\\nBreaks down tasks into subtasks\\nDelegates subtasks to workers\\nSynthesizes worker outputs into a final result\\n\\n\\nOrchestrator-worker workflows provide more flexibility and are often used when subtasks cannot be predefined the way they can with parallelization. This is common with workflows that write code or need to update content across multiple files. For example, a workflow that needs to update installation instructions for multiple Python libraries across an unknown number of documents might use this pattern.\\nGraph APIFunctional APICopyfrom typing import Annotated, List\\nimport operator\\n\\n\\n# Schema for structured output to use in planning\\nclass Section(BaseModel):\\n    name: str = Field(\\n        description=\"Name for this section of the report.\",\\n    )\\n    description: str = Field(\\n        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\\n    )\\n\\n\\nclass Sections(BaseModel):\\n    sections: List[Section] = Field(\\n        description=\"Sections of the report.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nplanner = llm.with_structured_output(Sections)\\n\\n\\u200bCreating workers in LangGraph\\nOrchestrator-worker workflows are common and LangGraph has built-in support for them. The Send API lets you dynamically create worker nodes and send them specific inputs. Each worker has its own state, and all worker outputs are written to a shared state key that is accessible to the orchestrator graph. This gives the orchestrator access to all worker output and allows it to synthesize them into a final output. The example below iterates over a list of sections and uses the Send API to send a section to each worker.\\nCopyfrom langgraph.types import Send\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str  # Report topic\\n    sections: list[Section]  # List of report sections\\n    completed_sections: Annotated[\\n        list, operator.add\\n    ]  # All workers write to this key in parallel\\n    final_report: str  # Final report\\n\\n\\n# Worker state\\nclass WorkerState(TypedDict):\\n    section: Section\\n    completed_sections: Annotated[list, operator.add]\\n\\n\\n# Nodes\\ndef orchestrator(state: State):\\n    \"\"\"Orchestrator that generates a plan for the report\"\"\"\\n\\n    # Generate queries\\n    report_sections = planner.invoke(\\n        [\\n            SystemMessage(content=\"Generate a plan for the report.\"),\\n            HumanMessage(content=f\"Here is the report topic: {state[\\'topic\\']}\"),\\n        ]\\n    )\\n\\n    return {\"sections\": report_sections.sections}\\n\\n\\ndef llm_call(state: WorkerState):\\n    \"\"\"Worker writes a section of the report\"\"\"\\n\\n    # Generate section\\n    section = llm.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Write a report section following the provided name and description. Include no preamble for each section. Use markdown formatting.\"\\n            ),\\n            HumanMessage(\\n                content=f\"Here is the section name: {state[\\'section\\'].name} and description: {state[\\'section\\'].description}\"\\n            ),\\n        ]\\n    )\\n\\n    # Write the updated section to completed sections\\n    return {\"completed_sections\": [section.content]}\\n\\n\\ndef synthesizer(state: State):\\n    \"\"\"Synthesize full report from sections\"\"\"\\n\\n    # List of completed sections\\n    completed_sections = state[\"completed_sections\"]\\n\\n    # Format completed section to str to use as context for final sections\\n    completed_report_sections = \"\\\\n\\\\n---\\\\n\\\\n\".join(completed_sections)\\n\\n    return {\"final_report\": completed_report_sections}\\n\\n\\n# Conditional edge function to create llm_call workers that each write a section of the report\\ndef assign_workers(state: State):\\n    \"\"\"Assign a worker to each section in the plan\"\"\"\\n\\n    # Kick off section writing in parallel via Send() API\\n    return [Send(\"llm_call\", {\"section\": s}) for s in state[\"sections\"]]\\n\\n\\n# Build workflow\\norchestrator_worker_builder = StateGraph(State)\\n\\n# Add the nodes\\norchestrator_worker_builder.add_node(\"orchestrator\", orchestrator)\\norchestrator_worker_builder.add_node(\"llm_call\", llm_call)\\norchestrator_worker_builder.add_node(\"synthesizer\", synthesizer)\\n\\n# Add edges to connect nodes\\norchestrator_worker_builder.add_edge(START, \"orchestrator\")\\norchestrator_worker_builder.add_conditional_edges(\\n    \"orchestrator\", assign_workers, [\"llm_call\"]\\n)\\norchestrator_worker_builder.add_edge(\"llm_call\", \"synthesizer\")\\norchestrator_worker_builder.add_edge(\"synthesizer\", END)\\n\\n# Compile the workflow\\norchestrator_worker = orchestrator_worker_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(orchestrator_worker.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = orchestrator_worker.invoke({\"topic\": \"Create a report on LLM scaling laws\"})\\n\\nfrom IPython.display import Markdown\\nMarkdown(state[\"final_report\"])\\n\\n\\u200bEvaluator-optimizer\\nIn evaluator-optimizer workflows, one LLM call creates a response and the other evaluates that response. If the evaluator or a human-in-the-loop determines the response needs refinement, feedback is provided and the response is recreated. This loop continues until an acceptable response is generated.\\nEvaluator-optimizer workflows are commonly used when there‚Äôs particular success criteria for a task, but iteration is required to meet that criteria. For example, there‚Äôs not always a perfect match when translating text between two languages. It might take a few iterations to generate a translation with the same meaning across the two languages.\\n\\nGraph APIFunctional APICopy# Graph state\\nclass State(TypedDict):\\n    joke: str\\n    topic: str\\n    feedback: str\\n    funny_or_not: str\\n\\n\\n# Schema for structured output to use in evaluation\\nclass Feedback(BaseModel):\\n    grade: Literal[\"funny\", \"not funny\"] = Field(\\n        description=\"Decide if the joke is funny or not.\",\\n    )\\n    feedback: str = Field(\\n        description=\"If the joke is not funny, provide feedback on how to improve it.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nevaluator = llm.with_structured_output(Feedback)\\n\\n\\n# Nodes\\ndef llm_call_generator(state: State):\\n    \"\"\"LLM generates a joke\"\"\"\\n\\n    if state.get(\"feedback\"):\\n        msg = llm.invoke(\\n            f\"Write a joke about {state[\\'topic\\']} but take into account the feedback: {state[\\'feedback\\']}\"\\n        )\\n    else:\\n        msg = llm.invoke(f\"Write a joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef llm_call_evaluator(state: State):\\n    \"\"\"LLM evaluates the joke\"\"\"\\n\\n    grade = evaluator.invoke(f\"Grade the joke {state[\\'joke\\']}\")\\n    return {\"funny_or_not\": grade.grade, \"feedback\": grade.feedback}\\n\\n\\n# Conditional edge function to route back to joke generator or end based upon feedback from the evaluator\\ndef route_joke(state: State):\\n    \"\"\"Route back to joke generator or end based upon feedback from the evaluator\"\"\"\\n\\n    if state[\"funny_or_not\"] == \"funny\":\\n        return \"Accepted\"\\n    elif state[\"funny_or_not\"] == \"not funny\":\\n        return \"Rejected + Feedback\"\\n\\n\\n# Build workflow\\noptimizer_builder = StateGraph(State)\\n\\n# Add the nodes\\noptimizer_builder.add_node(\"llm_call_generator\", llm_call_generator)\\noptimizer_builder.add_node(\"llm_call_evaluator\", llm_call_evaluator)\\n\\n# Add edges to connect nodes\\noptimizer_builder.add_edge(START, \"llm_call_generator\")\\noptimizer_builder.add_edge(\"llm_call_generator\", \"llm_call_evaluator\")\\noptimizer_builder.add_conditional_edges(\\n    \"llm_call_evaluator\",\\n    route_joke,\\n    {  # Name returned by route_joke : Name of next node to visit\\n        \"Accepted\": END,\\n        \"Rejected + Feedback\": \"llm_call_generator\",\\n    },\\n)\\n\\n# Compile the workflow\\noptimizer_workflow = optimizer_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(optimizer_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = optimizer_workflow.invoke({\"topic\": \"Cats\"})\\nprint(state[\"joke\"])\\n\\n\\u200bAgents\\nAgents are typically implemented as an LLM performing actions using tools. They operate in continuous feedback loops, and are used in situations where problems and solutions are unpredictable. Agents have more autonomy than workflows, and can make decisions about the tools they use and how to solve problems. You can still define the available toolset and guidelines for how agents behave.\\n\\nTo get started with agents, see the quickstart or read more about how they work in LangChain.\\nUsing toolsCopyfrom langchain.tools import tool\\n\\n\\n# Define tools\\n@tool\\ndef multiply(a: int, b: int) -> int:\\n    \"\"\"Multiply `a` and `b`.\\n\\n    Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a * b\\n\\n\\n@tool\\ndef add(a: int, b: int) -> int:\\n    \"\"\"Adds `a` and `b`.\\n\\n    Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a + b\\n\\n\\n@tool\\ndef divide(a: int, b: int) -> float:\\n    \"\"\"Divide `a` and `b`.\\n\\n    Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a / b\\n\\n\\n# Augment the LLM with tools\\ntools = [add, multiply, divide]\\ntools_by_name = {tool.name: tool for tool in tools}\\nllm_with_tools = llm.bind_tools(tools)\\n\\nGraph APIFunctional APICopyfrom langgraph.graph import MessagesState\\nfrom langchain.messages import SystemMessage, HumanMessage, ToolMessage\\n\\n\\n# Nodes\\ndef llm_call(state: MessagesState):\\n    \"\"\"LLM decides whether to call a tool or not\"\"\"\\n\\n    return {\\n        \"messages\": [\\n            llm_with_tools.invoke(\\n                [\\n                    SystemMessage(\\n                        content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\\n                    )\\n                ]\\n                + state[\"messages\"]\\n            )\\n        ]\\n    }\\n\\n\\ndef tool_node(state: dict):\\n    \"\"\"Performs the tool call\"\"\"\\n\\n    result = []\\n    for tool_call in state[\"messages\"][-1].tool_calls:\\n        tool = tools_by_name[tool_call[\"name\"]]\\n        observation = tool.invoke(tool_call[\"args\"])\\n        result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\\n    return {\"messages\": result}\\n\\n\\n# Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call\\ndef should_continue(state: MessagesState) -> Literal[\"tool_node\", END]:\\n    \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\\n\\n    messages = state[\"messages\"]\\n    last_message = messages[-1]\\n\\n    # If the LLM makes a tool call, then perform an action\\n    if last_message.tool_calls:\\n        return \"tool_node\"\\n\\n    # Otherwise, we stop (reply to the user)\\n    return END\\n\\n\\n# Build workflow\\nagent_builder = StateGraph(MessagesState)\\n\\n# Add nodes\\nagent_builder.add_node(\"llm_call\", llm_call)\\nagent_builder.add_node(\"tool_node\", tool_node)\\n\\n# Add edges to connect nodes\\nagent_builder.add_edge(START, \"llm_call\")\\nagent_builder.add_conditional_edges(\\n    \"llm_call\",\\n    should_continue,\\n    [\"tool_node\", END]\\n)\\nagent_builder.add_edge(\"tool_node\", \"llm_call\")\\n\\n# Compile the agent\\nagent = agent_builder.compile()\\n\\n# Show the agent\\ndisplay(Image(agent.get_graph(xray=True).draw_mermaid_png()))\\n\\n# Invoke\\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\\nmessages = agent.invoke({\"messages\": messages})\\nfor m in messages[\"messages\"]:\\n    m.pretty_print()\\n\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoThinking in LangGraphPreviousPersistenceNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/use-graph-api#map-reduce-and-the-send-api', 'title': 'Use the graph API - Docs by LangChain', 'language': 'en'}, page_content='Use the graph API - Docs by LangChainSkip to main contentüöÄ Share how you\\'re building agents for a chance to win LangChain swag!Docs by LangChain home pageLangChain + LangGraphSearch...‚åòKAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationGraph APIUse the graph APILangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureTestLangSmith StudioAgent Chat UILangSmith DeploymentLangSmith ObservabilityLangGraph APIsGraph APIChoosing APIsGraph APIUse the graph APIFunctional APIRuntimeOn this pageSetupDefine and update stateDefine stateUpdate stateProcess state updates with reducersMessagesStateBypass reducers with OverwriteDefine input and output schemasPass private state between nodesUse Pydantic models for graph stateAdd runtime configurationAdd retry policiesAdd node cachingCreate a sequence of stepsCreate branchesRun graph nodes in parallelDefer node executionConditional branchingMap-Reduce and the Send APICreate and control loopsImpose a recursion limitAsyncCombine control flow and state updates with CommandNavigate to a node in a parent graphUse inside toolsVisualize your graphMermaidPNGLangGraph APIsGraph APIUse the graph APICopy pageCopy pageThis guide demonstrates the basics of LangGraph‚Äôs Graph API. It walks through state, as well as composing common graph structures such as sequences, branches, and loops. It also covers LangGraph‚Äôs control features, including the Send API for map-reduce workflows and the Command API for combining state updates with ‚Äúhops‚Äù across nodes.\\n\\u200bSetup\\nInstall langgraph:\\npipuvCopypip install -U langgraph\\n\\nSet up LangSmith for better debuggingSign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph ‚Äî read more about how to get started in the docs.\\n\\u200bDefine and update state\\nHere we show how to define and update state in LangGraph. We will demonstrate:\\n\\nHow to use state to define a graph‚Äôs schema\\nHow to use reducers to control how state updates are processed.\\n\\n\\u200bDefine state\\nState in LangGraph can be a TypedDict, Pydantic model, or dataclass. Below we will use TypedDict. See this section for detail on using Pydantic.\\nBy default, graphs will have the same input and output schema, and the state determines that schema. See this section for how to define distinct input and output schemas.\\nLet‚Äôs consider a simple example using messages. This represents a versatile formulation of state for many LLM applications. See our concepts page for more detail.\\nCopyfrom langchain.messages import AnyMessage\\nfrom typing_extensions import TypedDict\\n\\nclass State(TypedDict):\\n    messages: list[AnyMessage]\\n    extra_field: int\\n\\nThis state tracks a list of message objects, as well as an extra integer field.\\n\\u200bUpdate state\\nLet‚Äôs build an example graph with a single node. Our node is just a Python function that reads our graph‚Äôs state and makes updates to it. The first argument to this function will always be the state:\\nCopyfrom langchain.messages import AIMessage\\n\\ndef node(state: State):\\n    messages = state[\"messages\"]\\n    new_message = AIMessage(\"Hello!\")\\n    return {\"messages\": messages + [new_message], \"extra_field\": 10}\\n\\nThis node simply appends a message to our message list, and populates an extra field.\\nNodes should return updates to the state directly, instead of mutating the state.\\nLet‚Äôs next define a simple graph containing this node. We use StateGraph to define a graph that operates on this state. We then use add_node populate our graph.\\nCopyfrom langgraph.graph import StateGraph\\n\\nbuilder = StateGraph(State)\\nbuilder.add_node(node)\\nbuilder.set_entry_point(\"node\")\\ngraph = builder.compile()\\n\\nLangGraph provides built-in utilities for visualizing your graph. Let‚Äôs inspect our graph. See this section for detail on visualization.\\nCopyfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nIn this case, our graph just executes a single node. Let‚Äôs proceed with a simple invocation:\\nCopyfrom langchain.messages import HumanMessage\\n\\nresult = graph.invoke({\"messages\": [HumanMessage(\"Hi\")]})\\nresult\\n\\nCopy{\\'messages\\': [HumanMessage(content=\\'Hi\\'), AIMessage(content=\\'Hello!\\')], \\'extra_field\\': 10}\\n\\nNote that:\\n\\nWe kicked off invocation by updating a single key of the state.\\nWe receive the entire state in the invocation result.\\n\\nFor convenience, we frequently inspect the content of message objects via pretty-print:\\nCopyfor message in result[\"messages\"]:\\n    message.pretty_print()\\n\\nCopy================================ Human Message ================================\\n\\nHi\\n================================== Ai Message ==================================\\n\\nHello!\\n\\n\\u200bProcess state updates with reducers\\nEach key in the state can have its own independent reducer function, which controls how updates from nodes are applied. If no reducer function is explicitly specified then it is assumed that all updates to the key should override it.\\nFor TypedDict state schemas, we can define reducers by annotating the corresponding field of the state with a reducer function.\\nIn the earlier example, our node updated the \"messages\" key in the state by appending a message to it. Below, we add a reducer to this key, such that updates are automatically appended:\\nCopyfrom typing_extensions import Annotated\\n\\ndef add(left, right):\\n    \"\"\"Can also import `add` from the `operator` built-in.\"\"\"\\n    return left + right\\n\\nclass State(TypedDict):\\n    messages: Annotated[list[AnyMessage], add]  \\n    extra_field: int\\n\\nNow our node can be simplified:\\nCopydef node(state: State):\\n    new_message = AIMessage(\"Hello!\")\\n    return {\"messages\": [new_message], \"extra_field\": 10}  \\n\\nCopyfrom langgraph.graph import START\\n\\ngraph = StateGraph(State).add_node(node).add_edge(START, \"node\").compile()\\n\\nresult = graph.invoke({\"messages\": [HumanMessage(\"Hi\")]})\\n\\nfor message in result[\"messages\"]:\\n    message.pretty_print()\\n\\nCopy================================ Human Message ================================\\n\\nHi\\n================================== Ai Message ==================================\\n\\nHello!\\n\\n\\u200bMessagesState\\nIn practice, there are additional considerations for updating lists of messages:\\n\\nWe may wish to update an existing message in the state.\\nWe may want to accept short-hands for message formats, such as OpenAI format.\\n\\nLangGraph includes a built-in reducer add_messages that handles these considerations:\\nCopyfrom langgraph.graph.message import add_messages\\n\\nclass State(TypedDict):\\n    messages: Annotated[list[AnyMessage], add_messages]  \\n    extra_field: int\\n\\ndef node(state: State):\\n    new_message = AIMessage(\"Hello!\")\\n    return {\"messages\": [new_message], \"extra_field\": 10}\\n\\ngraph = StateGraph(State).add_node(node).set_entry_point(\"node\").compile()\\n\\nCopyinput_message = {\"role\": \"user\", \"content\": \"Hi\"}  \\n\\nresult = graph.invoke({\"messages\": [input_message]})\\n\\nfor message in result[\"messages\"]:\\n    message.pretty_print()\\n\\nCopy================================ Human Message ================================\\n\\nHi\\n================================== Ai Message ==================================\\n\\nHello!\\n\\nThis is a versatile representation of state for applications involving chat models. LangGraph includes a pre-built MessagesState for convenience, so that we can have:\\nCopyfrom langgraph.graph import MessagesState\\n\\nclass State(MessagesState):\\n    extra_field: int\\n\\n\\u200bBypass reducers with Overwrite\\nIn some cases, you may want to bypass a reducer and directly overwrite a state value. LangGraph provides the Overwrite type for this purpose. When a node returns a value wrapped with Overwrite, the reducer is bypassed and the channel is set directly to that value.\\nThis is useful when you want to reset or replace accumulated state rather than merge it with existing values.\\nCopyfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.types import Overwrite\\nfrom typing_extensions import Annotated, TypedDict\\nimport operator\\n\\nclass State(TypedDict):\\n    messages: Annotated[list, operator.add]\\n\\ndef add_message(state: State):\\n    return {\"messages\": [\"first message\"]}\\n\\ndef replace_messages(state: State):\\n    # Bypass the reducer and replace the entire messages list\\n    return {\"messages\": Overwrite([\"replacement message\"])}\\n\\nbuilder = StateGraph(State)\\nbuilder.add_node(\"add_message\", add_message)\\nbuilder.add_node(\"replace_messages\", replace_messages)\\nbuilder.add_edge(START, \"add_message\")\\nbuilder.add_edge(\"add_message\", \"replace_messages\")\\nbuilder.add_edge(\"replace_messages\", END)\\n\\ngraph = builder.compile()\\n\\nresult = graph.invoke({\"messages\": [\"initial\"]})\\nprint(result[\"messages\"])\\n\\nCopy[\\'replacement message\\']\\n\\nYou can also use JSON format with the special key \"__overwrite__\":\\nCopydef replace_messages(state: State):\\n    return {\"messages\": {\"__overwrite__\": [\"replacement message\"]}}\\n\\nWhen nodes execute in parallel, only one node can use Overwrite on the same state key in a given super-step. If multiple nodes attempt to overwrite the same key in the same super-step, an InvalidUpdateError will be raised.\\n\\u200bDefine input and output schemas\\nBy default, StateGraph operates with a single schema, and all nodes are expected to communicate using that schema. However, it‚Äôs also possible to define distinct input and output schemas for a graph.\\nWhen distinct schemas are specified, an internal schema will still be used for communication between nodes. The input schema ensures that the provided input matches the expected structure, while the output schema filters the internal data to return only the relevant information according to the defined output schema.\\nBelow, we‚Äôll see how to define distinct input and output schema.\\nCopyfrom langgraph.graph import StateGraph, START, END\\nfrom typing_extensions import TypedDict\\n\\n# Define the schema for the input\\nclass InputState(TypedDict):\\n    question: str\\n\\n# Define the schema for the output\\nclass OutputState(TypedDict):\\n    answer: str\\n\\n# Define the overall schema, combining both input and output\\nclass OverallState(InputState, OutputState):\\n    pass\\n\\n# Define the node that processes the input and generates an answer\\ndef answer_node(state: InputState):\\n    # Example answer and an extra key\\n    return {\"answer\": \"bye\", \"question\": state[\"question\"]}\\n\\n# Build the graph with input and output schemas specified\\nbuilder = StateGraph(OverallState, input_schema=InputState, output_schema=OutputState)\\nbuilder.add_node(answer_node)  # Add the answer node\\nbuilder.add_edge(START, \"answer_node\")  # Define the starting edge\\nbuilder.add_edge(\"answer_node\", END)  # Define the ending edge\\ngraph = builder.compile()  # Compile the graph\\n\\n# Invoke the graph with an input and print the result\\nprint(graph.invoke({\"question\": \"hi\"}))\\n\\nCopy{\\'answer\\': \\'bye\\'}\\n\\nNotice that the output of invoke only includes the output schema.\\n\\u200bPass private state between nodes\\nIn some cases, you may want nodes to exchange information that is crucial for intermediate logic but doesn‚Äôt need to be part of the main schema of the graph. This private data is not relevant to the overall input/output of the graph and should only be shared between certain nodes.\\nBelow, we‚Äôll create an example sequential graph consisting of three nodes (node_1, node_2 and node_3), where private data is passed between the first two steps (node_1 and node_2), while the third step (node_3) only has access to the public overall state.\\nCopyfrom langgraph.graph import StateGraph, START, END\\nfrom typing_extensions import TypedDict\\n\\n# The overall state of the graph (this is the public state shared across nodes)\\nclass OverallState(TypedDict):\\n    a: str\\n\\n# Output from node_1 contains private data that is not part of the overall state\\nclass Node1Output(TypedDict):\\n    private_data: str\\n\\n# The private data is only shared between node_1 and node_2\\ndef node_1(state: OverallState) -> Node1Output:\\n    output = {\"private_data\": \"set by node_1\"}\\n    print(f\"Entered node `node_1`:\\\\n\\\\tInput: {state}.\\\\n\\\\tReturned: {output}\")\\n    return output\\n\\n# Node 2 input only requests the private data available after node_1\\nclass Node2Input(TypedDict):\\n    private_data: str\\n\\ndef node_2(state: Node2Input) -> OverallState:\\n    output = {\"a\": \"set by node_2\"}\\n    print(f\"Entered node `node_2`:\\\\n\\\\tInput: {state}.\\\\n\\\\tReturned: {output}\")\\n    return output\\n\\n# Node 3 only has access to the overall state (no access to private data from node_1)\\ndef node_3(state: OverallState) -> OverallState:\\n    output = {\"a\": \"set by node_3\"}\\n    print(f\"Entered node `node_3`:\\\\n\\\\tInput: {state}.\\\\n\\\\tReturned: {output}\")\\n    return output\\n\\n# Connect nodes in a sequence\\n# node_2 accepts private data from node_1, whereas\\n# node_3 does not see the private data.\\nbuilder = StateGraph(OverallState).add_sequence([node_1, node_2, node_3])\\nbuilder.add_edge(START, \"node_1\")\\ngraph = builder.compile()\\n\\n# Invoke the graph with the initial state\\nresponse = graph.invoke(\\n    {\\n        \"a\": \"set at start\",\\n    }\\n)\\n\\nprint()\\nprint(f\"Output of graph invocation: {response}\")\\n\\nCopyEntered node `node_1`:\\n    Input: {\\'a\\': \\'set at start\\'}.\\n    Returned: {\\'private_data\\': \\'set by node_1\\'}\\nEntered node `node_2`:\\n    Input: {\\'private_data\\': \\'set by node_1\\'}.\\n    Returned: {\\'a\\': \\'set by node_2\\'}\\nEntered node `node_3`:\\n    Input: {\\'a\\': \\'set by node_2\\'}.\\n    Returned: {\\'a\\': \\'set by node_3\\'}\\n\\nOutput of graph invocation: {\\'a\\': \\'set by node_3\\'}\\n\\n\\u200bUse Pydantic models for graph state\\nA StateGraph accepts a state_schema argument on initialization that specifies the ‚Äúshape‚Äù of the state that the nodes in the graph can access and update.\\nIn our examples, we typically use a python-native TypedDict or dataclass for state_schema, but state_schema can be any type.\\nHere, we‚Äôll see how a Pydantic BaseModel can be used for state_schema to add run-time validation on inputs.\\nKnown Limitations\\nCurrently, the output of the graph will NOT be an instance of a pydantic model.\\nRun-time validation only occurs on inputs to the first node in the graph, not on subsequent nodes or outputs.\\nThe validation error trace from pydantic does not show which node the error arises in.\\nPydantic‚Äôs recursive validation can be slow. For performance-sensitive applications, you may want to consider using a dataclass instead.\\n\\nCopyfrom langgraph.graph import StateGraph, START, END\\nfrom typing_extensions import TypedDict\\nfrom pydantic import BaseModel\\n\\n# The overall state of the graph (this is the public state shared across nodes)\\nclass OverallState(BaseModel):\\n    a: str\\n\\ndef node(state: OverallState):\\n    return {\"a\": \"goodbye\"}\\n\\n# Build the state graph\\nbuilder = StateGraph(OverallState)\\nbuilder.add_node(node)  # node_1 is the first node\\nbuilder.add_edge(START, \"node\")  # Start the graph with node_1\\nbuilder.add_edge(\"node\", END)  # End the graph after node_1\\ngraph = builder.compile()\\n\\n# Test the graph with a valid input\\ngraph.invoke({\"a\": \"hello\"})\\n\\nInvoke the graph with an invalid input\\nCopytry:\\n    graph.invoke({\"a\": 123})  # Should be a string\\nexcept Exception as e:\\n    print(\"An exception was raised because `a` is an integer rather than a string.\")\\n    print(e)\\n\\nCopyAn exception was raised because `a` is an integer rather than a string.\\n1 validation error for OverallState\\na\\n  Input should be a valid string [type=string_type, input_value=123, input_type=int]\\n    For further information visit https://errors.pydantic.dev/2.9/v/string_type\\n\\nSee below for additional features of Pydantic model state:\\nSerialization BehaviorWhen using Pydantic models as state schemas, it‚Äôs important to understand how serialization works, especially when:\\nPassing Pydantic objects as inputs\\nReceiving outputs from the graph\\nWorking with nested Pydantic models\\nLet‚Äôs see these behaviors in action.Copyfrom langgraph.graph import StateGraph, START, END\\nfrom pydantic import BaseModel\\n\\nclass NestedModel(BaseModel):\\n    value: str\\n\\nclass ComplexState(BaseModel):\\n    text: str\\n    count: int\\n    nested: NestedModel\\n\\ndef process_node(state: ComplexState):\\n    # Node receives a validated Pydantic object\\n    print(f\"Input state type: {type(state)}\")\\n    print(f\"Nested type: {type(state.nested)}\")\\n    # Return a dictionary update\\n    return {\"text\": state.text + \" processed\", \"count\": state.count + 1}\\n\\n# Build the graph\\nbuilder = StateGraph(ComplexState)\\nbuilder.add_node(\"process\", process_node)\\nbuilder.add_edge(START, \"process\")\\nbuilder.add_edge(\"process\", END)\\ngraph = builder.compile()\\n\\n# Create a Pydantic instance for input\\ninput_state = ComplexState(text=\"hello\", count=0, nested=NestedModel(value=\"test\"))\\nprint(f\"Input object type: {type(input_state)}\")\\n\\n# Invoke graph with a Pydantic instance\\nresult = graph.invoke(input_state)\\nprint(f\"Output type: {type(result)}\")\\nprint(f\"Output content: {result}\")\\n\\n# Convert back to Pydantic model if needed\\noutput_model = ComplexState(**result)\\nprint(f\"Converted back to Pydantic: {type(output_model)}\")\\n\\nRuntime Type CoercionPydantic performs runtime type coercion for certain data types. This can be helpful but also lead to unexpected behavior if you‚Äôre not aware of it.Copyfrom langgraph.graph import StateGraph, START, END\\nfrom pydantic import BaseModel\\n\\nclass CoercionExample(BaseModel):\\n    # Pydantic will coerce string numbers to integers\\n    number: int\\n    # Pydantic will parse string booleans to bool\\n    flag: bool\\n\\ndef inspect_node(state: CoercionExample):\\n    print(f\"number: {state.number} (type: {type(state.number)})\")\\n    print(f\"flag: {state.flag} (type: {type(state.flag)})\")\\n    return {}\\n\\nbuilder = StateGraph(CoercionExample)\\nbuilder.add_node(\"inspect\", inspect_node)\\nbuilder.add_edge(START, \"inspect\")\\nbuilder.add_edge(\"inspect\", END)\\ngraph = builder.compile()\\n\\n# Demonstrate coercion with string inputs that will be converted\\nresult = graph.invoke({\"number\": \"42\", \"flag\": \"true\"})\\n\\n# This would fail with a validation error\\ntry:\\n    graph.invoke({\"number\": \"not-a-number\", \"flag\": \"true\"})\\nexcept Exception as e:\\n    print(f\"\\\\nExpected validation error: {e}\")\\n\\nWorking with Message ModelsWhen working with LangChain message types in your state schema, there are important considerations for serialization. You should use AnyMessage (rather than BaseMessage) for proper serialization/deserialization when using message objects over the wire.Copyfrom langgraph.graph import StateGraph, START, END\\nfrom pydantic import BaseModel\\nfrom langchain.messages import HumanMessage, AIMessage, AnyMessage\\nfrom typing import List\\n\\nclass ChatState(BaseModel):\\n    messages: List[AnyMessage]\\n    context: str\\n\\ndef add_message(state: ChatState):\\n    return {\"messages\": state.messages + [AIMessage(content=\"Hello there!\")]}\\n\\nbuilder = StateGraph(ChatState)\\nbuilder.add_node(\"add_message\", add_message)\\nbuilder.add_edge(START, \"add_message\")\\nbuilder.add_edge(\"add_message\", END)\\ngraph = builder.compile()\\n\\n# Create input with a message\\ninitial_state = ChatState(\\n    messages=[HumanMessage(content=\"Hi\")], context=\"Customer support chat\"\\n)\\n\\nresult = graph.invoke(initial_state)\\nprint(f\"Output: {result}\")\\n\\n# Convert back to Pydantic model to see message types\\noutput_model = ChatState(**result)\\nfor i, msg in enumerate(output_model.messages):\\n    print(f\"Message {i}: {type(msg).__name__} - {msg.content}\")\\n\\n\\u200bAdd runtime configuration\\nSometimes you want to be able to configure your graph when calling it. For example, you might want to be able to specify what LLM or system prompt to use at runtime, without polluting the graph state with these parameters.\\nTo add runtime configuration:\\n\\nSpecify a schema for your configuration\\nAdd the configuration to the function signature for nodes or conditional edges\\nPass the configuration into the graph.\\n\\nSee below for a simple example:\\nCopyfrom langgraph.graph import END, StateGraph, START\\nfrom langgraph.runtime import Runtime\\nfrom typing_extensions import TypedDict\\n\\n# 1. Specify config schema\\nclass ContextSchema(TypedDict):\\n    my_runtime_value: str\\n\\n# 2. Define a graph that accesses the config in a node\\nclass State(TypedDict):\\n    my_state_value: str\\n\\ndef node(state: State, runtime: Runtime[ContextSchema]):  \\n    if runtime.context[\"my_runtime_value\"] == \"a\":  \\n        return {\"my_state_value\": 1}\\n    elif runtime.context[\"my_runtime_value\"] == \"b\":  \\n        return {\"my_state_value\": 2}\\n    else:\\n        raise ValueError(\"Unknown values.\")\\n\\nbuilder = StateGraph(State, context_schema=ContextSchema)  \\nbuilder.add_node(node)\\nbuilder.add_edge(START, \"node\")\\nbuilder.add_edge(\"node\", END)\\n\\ngraph = builder.compile()\\n\\n# 3. Pass in configuration at runtime:\\nprint(graph.invoke({}, context={\"my_runtime_value\": \"a\"}))  \\nprint(graph.invoke({}, context={\"my_runtime_value\": \"b\"}))  \\n\\nCopy{\\'my_state_value\\': 1}\\n{\\'my_state_value\\': 2}\\n\\nExtended example: specifying LLM at runtimeBelow we demonstrate a practical example in which we configure what LLM to use at runtime. We will use both OpenAI and Anthropic models.Copyfrom dataclasses import dataclass\\n\\nfrom langchain.chat_models import init_chat_model\\nfrom langgraph.graph import MessagesState, END, StateGraph, START\\nfrom langgraph.runtime import Runtime\\nfrom typing_extensions import TypedDict\\n\\n@dataclass\\nclass ContextSchema:\\n    model_provider: str = \"anthropic\"\\n\\nMODELS = {\\n    \"anthropic\": init_chat_model(\"claude-haiku-4-5-20251001\"),\\n    \"openai\": init_chat_model(\"gpt-4.1-mini\"),\\n}\\n\\ndef call_model(state: MessagesState, runtime: Runtime[ContextSchema]):\\n    model = MODELS[runtime.context.model_provider]\\n    response = model.invoke(state[\"messages\"])\\n    return {\"messages\": [response]}\\n\\nbuilder = StateGraph(MessagesState, context_schema=ContextSchema)\\nbuilder.add_node(\"model\", call_model)\\nbuilder.add_edge(START, \"model\")\\nbuilder.add_edge(\"model\", END)\\n\\ngraph = builder.compile()\\n\\n# Usage\\ninput_message = {\"role\": \"user\", \"content\": \"hi\"}\\n# With no configuration, uses default (Anthropic)\\nresponse_1 = graph.invoke({\"messages\": [input_message]}, context=ContextSchema())[\"messages\"][-1]\\n# Or, can set OpenAI\\nresponse_2 = graph.invoke({\"messages\": [input_message]}, context={\"model_provider\": \"openai\"})[\"messages\"][-1]\\n\\nprint(response_1.response_metadata[\"model_name\"])\\nprint(response_2.response_metadata[\"model_name\"])\\nCopyclaude-haiku-4-5-20251001\\ngpt-4.1-mini-2025-04-14\\n\\nExtended example: specifying model and system message at runtimeBelow we demonstrate a practical example in which we configure two parameters: the LLM and system message to use at runtime.Copyfrom dataclasses import dataclass\\nfrom langchain.chat_models import init_chat_model\\nfrom langchain.messages import SystemMessage\\nfrom langgraph.graph import END, MessagesState, StateGraph, START\\nfrom langgraph.runtime import Runtime\\nfrom typing_extensions import TypedDict\\n\\n@dataclass\\nclass ContextSchema:\\n    model_provider: str = \"anthropic\"\\n    system_message: str | None = None\\n\\nMODELS = {\\n    \"anthropic\": init_chat_model(\"claude-haiku-4-5-20251001\"),\\n    \"openai\": init_chat_model(\"gpt-4.1-mini\"),\\n}\\n\\ndef call_model(state: MessagesState, runtime: Runtime[ContextSchema]):\\n    model = MODELS[runtime.context.model_provider]\\n    messages = state[\"messages\"]\\n    if (system_message := runtime.context.system_message):\\n        messages = [SystemMessage(system_message)] + messages\\n    response = model.invoke(messages)\\n    return {\"messages\": [response]}\\n\\nbuilder = StateGraph(MessagesState, context_schema=ContextSchema)\\nbuilder.add_node(\"model\", call_model)\\nbuilder.add_edge(START, \"model\")\\nbuilder.add_edge(\"model\", END)\\n\\ngraph = builder.compile()\\n\\n# Usage\\ninput_message = {\"role\": \"user\", \"content\": \"hi\"}\\nresponse = graph.invoke({\"messages\": [input_message]}, context={\"model_provider\": \"openai\", \"system_message\": \"Respond in Italian.\"})\\nfor message in response[\"messages\"]:\\n    message.pretty_print()\\nCopy================================ Human Message ================================\\n\\nhi\\n================================== Ai Message ==================================\\n\\nCiao! Come posso aiutarti oggi?\\n\\n\\u200bAdd retry policies\\nThere are many use cases where you may wish for your node to have a custom retry policy, for example if you are calling an API, querying a database, or calling an LLM, etc. LangGraph lets you add retry policies to nodes.\\nTo configure a retry policy, pass the retry_policy parameter to the add_node. The retry_policy parameter takes in a RetryPolicy named tuple object. Below we instantiate a RetryPolicy object with the default parameters and associate it with a node:\\nCopyfrom langgraph.types import RetryPolicy\\n\\nbuilder.add_node(\\n    \"node_name\",\\n    node_function,\\n    retry_policy=RetryPolicy(),\\n)\\n\\nBy default, the retry_on parameter uses the default_retry_on function, which retries on any exception except for the following:\\n\\nValueError\\nTypeError\\nArithmeticError\\nImportError\\nLookupError\\nNameError\\nSyntaxError\\nRuntimeError\\nReferenceError\\nStopIteration\\nStopAsyncIteration\\nOSError\\n\\nIn addition, for exceptions from popular http request libraries such as requests and httpx it only retries on 5xx status codes.\\nExtended example: customizing retry policiesConsider an example in which we are reading from a SQL database. Below we pass two different retry policies to nodes:Copyimport sqlite3\\nfrom typing_extensions import TypedDict\\nfrom langchain.chat_models import init_chat_model\\nfrom langgraph.graph import END, MessagesState, StateGraph, START\\nfrom langgraph.types import RetryPolicy\\nfrom langchain_community.utilities import SQLDatabase\\nfrom langchain.messages import AIMessage\\n\\ndb = SQLDatabase.from_uri(\"sqlite:///:memory:\")\\nmodel = init_chat_model(\"claude-haiku-4-5-20251001\")\\n\\ndef query_database(state: MessagesState):\\n    query_result = db.run(\"SELECT * FROM Artist LIMIT 10;\")\\n    return {\"messages\": [AIMessage(content=query_result)]}\\n\\ndef call_model(state: MessagesState):\\n    response = model.invoke(state[\"messages\"])\\n    return {\"messages\": [response]}\\n\\n# Define a new graph\\nbuilder = StateGraph(MessagesState)\\nbuilder.add_node(\\n    \"query_database\",\\n    query_database,\\n    retry_policy=RetryPolicy(retry_on=sqlite3.OperationalError),\\n)\\nbuilder.add_node(\"model\", call_model, retry_policy=RetryPolicy(max_attempts=5))\\nbuilder.add_edge(START, \"model\")\\nbuilder.add_edge(\"model\", \"query_database\")\\nbuilder.add_edge(\"query_database\", END)\\ngraph = builder.compile()\\n\\n\\u200bAdd node caching\\nNode caching is useful in cases where you want to avoid repeating operations, like when doing something expensive (either in terms of time or cost). LangGraph lets you add individualized caching policies to nodes in a graph.\\nTo configure a cache policy, pass the cache_policy parameter to the add_node function. In the following example, a CachePolicy object is instantiated with a time to live of 120 seconds and the default key_func generator. Then it is associated with a node:\\nCopyfrom langgraph.types import CachePolicy\\n\\nbuilder.add_node(\\n    \"node_name\",\\n    node_function,\\n    cache_policy=CachePolicy(ttl=120),\\n)\\n\\nThen, to enable node-level caching for a graph, set the cache argument when compiling the graph. The example below uses InMemoryCache to set up a graph with in-memory cache, but SqliteCache is also available.\\nCopyfrom langgraph.cache.memory import InMemoryCache\\n\\ngraph = builder.compile(cache=InMemoryCache())\\n\\n\\u200bCreate a sequence of steps\\nPrerequisites\\nThis guide assumes familiarity with the above section on state.\\nHere we demonstrate how to construct a simple sequence of steps. We will show:\\n\\nHow to build a sequential graph\\nBuilt-in short-hand for constructing similar graphs.\\n\\nTo add a sequence of nodes, we use the add_node and add_edge methods of our graph:\\nCopyfrom langgraph.graph import START, StateGraph\\n\\nbuilder = StateGraph(State)\\n\\n# Add nodes\\nbuilder.add_node(step_1)\\nbuilder.add_node(step_2)\\nbuilder.add_node(step_3)\\n\\n# Add edges\\nbuilder.add_edge(START, \"step_1\")\\nbuilder.add_edge(\"step_1\", \"step_2\")\\nbuilder.add_edge(\"step_2\", \"step_3\")\\n\\nWe can also use the built-in shorthand .add_sequence:\\nCopybuilder = StateGraph(State).add_sequence([step_1, step_2, step_3])\\nbuilder.add_edge(START, \"step_1\")\\n\\nWhy split application steps into a sequence with LangGraph?LangGraph makes it easy to add an underlying persistence layer to your application.\\nThis allows state to be checkpointed in between the execution of nodes, so your LangGraph nodes govern:\\nHow state updates are checkpointed\\nHow interruptions are resumed in human-in-the-loop workflows\\nHow we can ‚Äúrewind‚Äù and branch-off executions using LangGraph‚Äôs time travel features\\nThey also determine how execution steps are streamed, and how your application is visualized and debugged using Studio.Let‚Äôs demonstrate an end-to-end example. We will create a sequence of three steps:\\nPopulate a value in a key of the state\\nUpdate the same value\\nPopulate a different value\\nLet‚Äôs first define our state. This governs the schema of the graph, and can also specify how to apply updates. See this section for more detail.In our case, we will just keep track of two values:Copyfrom typing_extensions import TypedDict\\n\\nclass State(TypedDict):\\n    value_1: str\\n    value_2: int\\nOur nodes are just Python functions that read our graph‚Äôs state and make updates to it. The first argument to this function will always be the state:Copydef step_1(state: State):\\n    return {\"value_1\": \"a\"}\\n\\ndef step_2(state: State):\\n    current_value_1 = state[\"value_1\"]\\n    return {\"value_1\": f\"{current_value_1} b\"}\\n\\ndef step_3(state: State):\\n    return {\"value_2\": 10}\\nNote that when issuing updates to the state, each node can just specify the value of the key it wishes to update.By default, this will overwrite the value of the corresponding key. You can also use reducers to control how updates are processed‚Äî for example, you can append successive updates to a key instead. See this section for more detail.Finally, we define the graph. We use StateGraph to define a graph that operates on this state.We will then use add_node and add_edge to populate our graph and define its control flow.Copyfrom langgraph.graph import START, StateGraph\\n\\nbuilder = StateGraph(State)\\n\\n# Add nodes\\nbuilder.add_node(step_1)\\nbuilder.add_node(step_2)\\nbuilder.add_node(step_3)\\n\\n# Add edges\\nbuilder.add_edge(START, \"step_1\")\\nbuilder.add_edge(\"step_1\", \"step_2\")\\nbuilder.add_edge(\"step_2\", \"step_3\")\\nSpecifying custom names\\nYou can specify custom names for nodes using add_node:Copybuilder.add_node(\"my_node\", step_1)\\nNote that:\\nadd_edge takes the names of nodes, which for functions defaults to node.__name__.\\nWe must specify the entry point of the graph. For this we add an edge with the START node.\\nThe graph halts when there are no more nodes to execute.\\nWe next compile our graph. This provides a few basic checks on the structure of the graph (e.g., identifying orphaned nodes). If we were adding persistence to our application via a checkpointer, it would also be passed in here.Copygraph = builder.compile()\\nLangGraph provides built-in utilities for visualizing your graph. Let‚Äôs inspect our sequence. See this guide for detail on visualization.Copyfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\nLet‚Äôs proceed with a simple invocation:Copygraph.invoke({\"value_1\": \"c\"})\\nCopy{\\'value_1\\': \\'a b\\', \\'value_2\\': 10}\\nNote that:\\nWe kicked off invocation by providing a value for a single state key. We must always provide a value for at least one key.\\nThe value we passed in was overwritten by the first node.\\nThe second node updated the value.\\nThe third node populated a different value.\\nBuilt-in shorthand\\nlanggraph>=0.2.46 includes a built-in short-hand add_sequence for adding node sequences. You can compile the same graph as follows:Copybuilder = StateGraph(State).add_sequence([step_1, step_2, step_3])  \\nbuilder.add_edge(START, \"step_1\")\\n\\ngraph = builder.compile()\\n\\ngraph.invoke({\"value_1\": \"c\"})\\n\\n\\u200bCreate branches\\nParallel execution of nodes is essential to speed up overall graph operation. LangGraph offers native support for parallel execution of nodes, which can significantly enhance the performance of graph-based workflows. This parallelization is achieved through fan-out and fan-in mechanisms, utilizing both standard edges and conditional_edges. Below are some examples showing how to add create branching dataflows that work for you.\\n\\u200bRun graph nodes in parallel\\nIn this example, we fan out from Node A to B and C and then fan in to D. With our state, we specify the reducer add operation. This will combine or accumulate values for the specific key in the State, rather than simply overwriting the existing value. For lists, this means concatenating the new list with the existing list. See the above section on state reducers for more detail on updating state with reducers.\\nCopyimport operator\\nfrom typing import Annotated, Any\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\nclass State(TypedDict):\\n    # The operator.add reducer fn makes this append-only\\n    aggregate: Annotated[list, operator.add]\\n\\ndef a(state: State):\\n    print(f\\'Adding \"A\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"]}\\n\\ndef b(state: State):\\n    print(f\\'Adding \"B\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\ndef c(state: State):\\n    print(f\\'Adding \"C\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"C\"]}\\n\\ndef d(state: State):\\n    print(f\\'Adding \"D\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"D\"]}\\n\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\nbuilder.add_node(c)\\nbuilder.add_node(d)\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_edge(\"a\", \"b\")\\nbuilder.add_edge(\"a\", \"c\")\\nbuilder.add_edge(\"b\", \"d\")\\nbuilder.add_edge(\"c\", \"d\")\\nbuilder.add_edge(\"d\", END)\\ngraph = builder.compile()\\n\\nCopyfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nWith the reducer, you can see that the values added in each node are accumulated.\\nCopygraph.invoke({\"aggregate\": []}, {\"configurable\": {\"thread_id\": \"foo\"}})\\n\\nCopyAdding \"A\" to []\\nAdding \"B\" to [\\'A\\']\\nAdding \"C\" to [\\'A\\']\\nAdding \"D\" to [\\'A\\', \\'B\\', \\'C\\']\\n\\nIn the above example, nodes \"b\" and \"c\" are executed concurrently in the same superstep. Because they are in the same step, node \"d\" executes after both \"b\" and \"c\" are finished.Importantly, updates from a parallel superstep may not be ordered consistently. If you need a consistent, predetermined ordering of updates from a parallel superstep, you should write the outputs to a separate field in the state together with a value with which to order them.\\nException handling?LangGraph executes nodes within supersteps, meaning that while parallel branches are executed in parallel, the entire superstep is transactional. If any of these branches raises an exception, none of the updates are applied to the state (the entire superstep errors).Importantly, when using a checkpointer, results from successful nodes within a superstep are saved, and don‚Äôt repeat when resumed.If you have error-prone (perhaps want to handle flakey API calls), LangGraph provides two ways to address this:\\nYou can write regular python code within your node to catch and handle exceptions.\\nYou can set a retry_policy to direct the graph to retry nodes that raise certain types of exceptions. Only failing branches are retried, so you needn‚Äôt worry about performing redundant work.\\nTogether, these let you perform parallel execution and fully control exception handling.\\nSet max concurrency\\nYou can control the maximum number of concurrent tasks by setting max_concurrency in the configuration when invoking the graph.Copygraph.invoke({\"value_1\": \"c\"}, {\"configurable\": {\"max_concurrency\": 10}})\\n\\n\\u200bDefer node execution\\nDeferring node execution is useful when you want to delay the execution of a node until all other pending tasks are completed. This is particularly relevant when branches have different lengths, which is common in workflows like map-reduce flows.\\nThe above example showed how to fan-out and fan-in when each path was only one step. But what if one branch had more than one step? Let‚Äôs add a node \"b_2\" in the \"b\" branch:\\nCopyimport operator\\nfrom typing import Annotated, Any\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\nclass State(TypedDict):\\n    # The operator.add reducer fn makes this append-only\\n    aggregate: Annotated[list, operator.add]\\n\\ndef a(state: State):\\n    print(f\\'Adding \"A\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"]}\\n\\ndef b(state: State):\\n    print(f\\'Adding \"B\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\ndef b_2(state: State):\\n    print(f\\'Adding \"B_2\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B_2\"]}\\n\\ndef c(state: State):\\n    print(f\\'Adding \"C\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"C\"]}\\n\\ndef d(state: State):\\n    print(f\\'Adding \"D\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"D\"]}\\n\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\nbuilder.add_node(b_2)\\nbuilder.add_node(c)\\nbuilder.add_node(d, defer=True)  \\nbuilder.add_edge(START, \"a\")\\nbuilder.add_edge(\"a\", \"b\")\\nbuilder.add_edge(\"a\", \"c\")\\nbuilder.add_edge(\"b\", \"b_2\")\\nbuilder.add_edge(\"b_2\", \"d\")\\nbuilder.add_edge(\"c\", \"d\")\\nbuilder.add_edge(\"d\", END)\\ngraph = builder.compile()\\n\\nCopyfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nCopygraph.invoke({\"aggregate\": []})\\n\\nCopyAdding \"A\" to []\\nAdding \"B\" to [\\'A\\']\\nAdding \"C\" to [\\'A\\']\\nAdding \"B_2\" to [\\'A\\', \\'B\\', \\'C\\']\\nAdding \"D\" to [\\'A\\', \\'B\\', \\'C\\', \\'B_2\\']\\n\\nIn the above example, nodes \"b\" and \"c\" are executed concurrently in the same superstep. We set defer=True on node d so it will not execute until all pending tasks are finished. In this case, this means that \"d\" waits to execute until the entire \"b\" branch is finished.\\n\\u200bConditional branching\\nIf your fan-out should vary at runtime based on the state, you can use add_conditional_edges to select one or more paths using the graph state. See example below, where node a generates a state update that determines the following node.\\nCopyimport operator\\nfrom typing import Annotated, Literal, Sequence\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\nclass State(TypedDict):\\n    aggregate: Annotated[list, operator.add]\\n    # Add a key to the state. We will set this key to determine\\n    # how we branch.\\n    which: str\\n\\ndef a(state: State):\\n    print(f\\'Adding \"A\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"], \"which\": \"c\"}  \\n\\ndef b(state: State):\\n    print(f\\'Adding \"B\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\ndef c(state: State):\\n    print(f\\'Adding \"C\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"C\"]}\\n\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\nbuilder.add_node(c)\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_edge(\"b\", END)\\nbuilder.add_edge(\"c\", END)\\n\\ndef conditional_edge(state: State) -> Literal[\"b\", \"c\"]:\\n    # Fill in arbitrary logic here that uses the state\\n    # to determine the next node\\n    return state[\"which\"]\\n\\nbuilder.add_conditional_edges(\"a\", conditional_edge)  \\n\\ngraph = builder.compile()\\n\\nCopyfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nCopyresult = graph.invoke({\"aggregate\": []})\\nprint(result)\\n\\nCopyAdding \"A\" to []\\nAdding \"C\" to [\\'A\\']\\n{\\'aggregate\\': [\\'A\\', \\'C\\'], \\'which\\': \\'c\\'}\\n\\nYour conditional edges can route to multiple destination nodes. For example:Copydef route_bc_or_cd(state: State) -> Sequence[str]:\\n    if state[\"which\"] == \"cd\":\\n        return [\"c\", \"d\"]\\n    return [\"b\", \"c\"]\\n\\n\\u200bMap-Reduce and the Send API\\nLangGraph supports map-reduce and other advanced branching patterns using the Send API. Here is an example of how to use it:\\nCopyfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.types import Send\\nfrom typing_extensions import TypedDict, Annotated\\nimport operator\\n\\nclass OverallState(TypedDict):\\n    topic: str\\n    subjects: list[str]\\n    jokes: Annotated[list[str], operator.add]\\n    best_selected_joke: str\\n\\ndef generate_topics(state: OverallState):\\n    return {\"subjects\": [\"lions\", \"elephants\", \"penguins\"]}\\n\\ndef generate_joke(state: OverallState):\\n    joke_map = {\\n        \"lions\": \"Why don\\'t lions like fast food? Because they can\\'t catch it!\",\\n        \"elephants\": \"Why don\\'t elephants use computers? They\\'re afraid of the mouse!\",\\n        \"penguins\": \"Why don\\'t penguins like talking to strangers at parties? Because they find it hard to break the ice.\"\\n    }\\n    return {\"jokes\": [joke_map[state[\"subject\"]]]}\\n\\ndef continue_to_jokes(state: OverallState):\\n    return [Send(\"generate_joke\", {\"subject\": s}) for s in state[\"subjects\"]]\\n\\ndef best_joke(state: OverallState):\\n    return {\"best_selected_joke\": \"penguins\"}\\n\\nbuilder = StateGraph(OverallState)\\nbuilder.add_node(\"generate_topics\", generate_topics)\\nbuilder.add_node(\"generate_joke\", generate_joke)\\nbuilder.add_node(\"best_joke\", best_joke)\\nbuilder.add_edge(START, \"generate_topics\")\\nbuilder.add_conditional_edges(\"generate_topics\", continue_to_jokes, [\"generate_joke\"])\\nbuilder.add_edge(\"generate_joke\", \"best_joke\")\\nbuilder.add_edge(\"best_joke\", END)\\ngraph = builder.compile()\\n\\nCopyfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nCopy# Call the graph: here we call it to generate a list of jokes\\nfor step in graph.stream({\"topic\": \"animals\"}):\\n    print(step)\\n\\nCopy{\\'generate_topics\\': {\\'subjects\\': [\\'lions\\', \\'elephants\\', \\'penguins\\']}}\\n{\\'generate_joke\\': {\\'jokes\\': [\"Why don\\'t lions like fast food? Because they can\\'t catch it!\"]}}\\n{\\'generate_joke\\': {\\'jokes\\': [\"Why don\\'t elephants use computers? They\\'re afraid of the mouse!\"]}}\\n{\\'generate_joke\\': {\\'jokes\\': [\\'Why don\\'t penguins like talking to strangers at parties? Because they find it hard to break the ice.\\']}}\\n{\\'best_joke\\': {\\'best_selected_joke\\': \\'penguins\\'}}\\n\\n\\u200bCreate and control loops\\nWhen creating a graph with a loop, we require a mechanism for terminating execution. This is most commonly done by adding a conditional edge that routes to the END node once we reach some termination condition.\\nYou can also set the graph recursion limit when invoking or streaming the graph. The recursion limit sets the number of supersteps that the graph is allowed to execute before it raises an error. Read more about the concept of recursion limits here.\\nLet‚Äôs consider a simple graph with a loop to better understand how these mechanisms work.\\nTo return the last value of your state instead of receiving a recursion limit error, see the next section.\\nWhen creating a loop, you can include a conditional edge that specifies a termination condition:\\nCopybuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\n\\ndef route(state: State) -> Literal[\"b\", END]:\\n    if termination_condition(state):\\n        return END\\n    else:\\n        return \"b\"\\n\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_conditional_edges(\"a\", route)\\nbuilder.add_edge(\"b\", \"a\")\\ngraph = builder.compile()\\n\\nTo control the recursion limit, specify \"recursionLimit\" in the config. This will raise a GraphRecursionError, which you can catch and handle:\\nCopyfrom langgraph.errors import GraphRecursionError\\n\\ntry:\\n    graph.invoke(inputs, {\"recursion_limit\": 3})\\nexcept GraphRecursionError:\\n    print(\"Recursion Error\")\\n\\nLet‚Äôs define a graph with a simple loop. Note that we use a conditional edge to implement a termination condition.\\nCopyimport operator\\nfrom typing import Annotated, Literal\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\nclass State(TypedDict):\\n    # The operator.add reducer fn makes this append-only\\n    aggregate: Annotated[list, operator.add]\\n\\ndef a(state: State):\\n    print(f\\'Node A sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"]}\\n\\ndef b(state: State):\\n    print(f\\'Node B sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\n# Define nodes\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\n\\n# Define edges\\ndef route(state: State) -> Literal[\"b\", END]:\\n    if len(state[\"aggregate\"]) < 7:\\n        return \"b\"\\n    else:\\n        return END\\n\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_conditional_edges(\"a\", route)\\nbuilder.add_edge(\"b\", \"a\")\\ngraph = builder.compile()\\n\\nCopyfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nThis architecture is similar to a ReAct agent in which node \"a\" is a tool-calling model, and node \"b\" represents the tools.\\nIn our route conditional edge, we specify that we should end after the \"aggregate\" list in the state passes a threshold length.\\nInvoking the graph, we see that we alternate between nodes \"a\" and \"b\" before terminating once we reach the termination condition.\\nCopygraph.invoke({\"aggregate\": []})\\n\\nCopyNode A sees []\\nNode B sees [\\'A\\']\\nNode A sees [\\'A\\', \\'B\\']\\nNode B sees [\\'A\\', \\'B\\', \\'A\\']\\nNode A sees [\\'A\\', \\'B\\', \\'A\\', \\'B\\']\\nNode B sees [\\'A\\', \\'B\\', \\'A\\', \\'B\\', \\'A\\']\\nNode A sees [\\'A\\', \\'B\\', \\'A\\', \\'B\\', \\'A\\', \\'B\\']\\n\\n\\u200bImpose a recursion limit\\nIn some applications, we may not have a guarantee that we will reach a given termination condition. In these cases, we can set the graph‚Äôs recursion limit. This will raise a GraphRecursionError after a given number of supersteps. We can then catch and handle this exception:\\nCopyfrom langgraph.errors import GraphRecursionError\\n\\ntry:\\n    graph.invoke({\"aggregate\": []}, {\"recursion_limit\": 4})\\nexcept GraphRecursionError:\\n    print(\"Recursion Error\")\\n\\nCopyNode A sees []\\nNode B sees [\\'A\\']\\nNode C sees [\\'A\\', \\'B\\']\\nNode D sees [\\'A\\', \\'B\\']\\nNode A sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\']\\nRecursion Error\\n\\nExtended example: return state on hitting recursion limitInstead of raising GraphRecursionError, we can introduce a new key to the state that keeps track of the number of steps remaining until reaching the recursion limit. We can then use this key to determine if we should end the run.LangGraph implements a special RemainingSteps annotation. Under the hood, it creates a ManagedValue channel ‚Äî a state channel that will exist for the duration of our graph run and no longer.Copyimport operator\\nfrom typing import Annotated, Literal\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.managed.is_last_step import RemainingSteps\\n\\nclass State(TypedDict):\\n    aggregate: Annotated[list, operator.add]\\n    remaining_steps: RemainingSteps\\n\\ndef a(state: State):\\n    print(f\\'Node A sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"]}\\n\\ndef b(state: State):\\n    print(f\\'Node B sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\n# Define nodes\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\n\\n# Define edges\\ndef route(state: State) -> Literal[\"b\", END]:\\n    if state[\"remaining_steps\"] <= 2:\\n        return END\\n    else:\\n        return \"b\"\\n\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_conditional_edges(\"a\", route)\\nbuilder.add_edge(\"b\", \"a\")\\ngraph = builder.compile()\\n\\n# Test it out\\nresult = graph.invoke({\"aggregate\": []}, {\"recursion_limit\": 4})\\nprint(result)\\nCopyNode A sees []\\nNode B sees [\\'A\\']\\nNode A sees [\\'A\\', \\'B\\']\\n{\\'aggregate\\': [\\'A\\', \\'B\\', \\'A\\']}\\n\\nExtended example: loops with branchesTo better understand how the recursion limit works, let‚Äôs consider a more complex example. Below we implement a loop, but one step fans out into two nodes:Copyimport operator\\nfrom typing import Annotated, Literal\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\nclass State(TypedDict):\\n    aggregate: Annotated[list, operator.add]\\n\\ndef a(state: State):\\n    print(f\\'Node A sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"]}\\n\\ndef b(state: State):\\n    print(f\\'Node B sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\ndef c(state: State):\\n    print(f\\'Node C sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"C\"]}\\n\\ndef d(state: State):\\n    print(f\\'Node D sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"D\"]}\\n\\n# Define nodes\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\nbuilder.add_node(c)\\nbuilder.add_node(d)\\n\\n# Define edges\\ndef route(state: State) -> Literal[\"b\", END]:\\n    if len(state[\"aggregate\"]) < 7:\\n        return \"b\"\\n    else:\\n        return END\\n\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_conditional_edges(\"a\", route)\\nbuilder.add_edge(\"b\", \"c\")\\nbuilder.add_edge(\"b\", \"d\")\\nbuilder.add_edge([\"c\", \"d\"], \"a\")\\ngraph = builder.compile()\\nCopyfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\nThis graph looks complex, but can be conceptualized as loop of supersteps:\\nNode A\\nNode B\\nNodes C and D\\nNode A\\n‚Ä¶\\nWe have a loop of four supersteps, where nodes C and D are executed concurrently.Invoking the graph as before, we see that we complete two full ‚Äúlaps‚Äù before hitting the termination condition:Copyresult = graph.invoke({\"aggregate\": []})\\nCopyNode A sees []\\nNode B sees [\\'A\\']\\nNode D sees [\\'A\\', \\'B\\']\\nNode C sees [\\'A\\', \\'B\\']\\nNode A sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\']\\nNode B sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\', \\'A\\']\\nNode D sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\', \\'A\\', \\'B\\']\\nNode C sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\', \\'A\\', \\'B\\']\\nNode A sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\', \\'A\\', \\'B\\', \\'C\\', \\'D\\']\\nHowever, if we set the recursion limit to four, we only complete one lap because each lap is four supersteps:Copyfrom langgraph.errors import GraphRecursionError\\n\\ntry:\\n    result = graph.invoke({\"aggregate\": []}, {\"recursion_limit\": 4})\\nexcept GraphRecursionError:\\n    print(\"Recursion Error\")\\nCopyNode A sees []\\nNode B sees [\\'A\\']\\nNode C sees [\\'A\\', \\'B\\']\\nNode D sees [\\'A\\', \\'B\\']\\nNode A sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\']\\nRecursion Error\\n\\n\\u200bAsync\\nUsing the async programming paradigm can produce significant performance improvements when running IO-bound code concurrently (e.g., making concurrent API requests to a chat model provider).\\nTo convert a sync implementation of the graph to an async implementation, you will need to:\\n\\nUpdate nodes use async def instead of def.\\nUpdate the code inside to use await appropriately.\\nInvoke the graph with .ainvoke or .astream as desired.\\n\\nBecause many LangChain objects implement the Runnable Protocol which has async variants of all the sync methods it‚Äôs typically fairly quick to upgrade a sync graph to an async graph.\\nSee example below. To demonstrate async invocations of underlying LLMs, we will include a chat model:\\n OpenAI Anthropic Azure Google Gemini AWS Bedrocküëâ Read the OpenAI chat model integration docsCopypip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"gpt-4.1\")\\n\\nCopyfrom langchain.chat_models import init_chat_model\\nfrom langgraph.graph import MessagesState, StateGraph\\n\\nasync def node(state: MessagesState):  \\n    new_message = await llm.ainvoke(state[\"messages\"])  \\n    return {\"messages\": [new_message]}\\n\\nbuilder = StateGraph(MessagesState).add_node(node).set_entry_point(\"node\")\\ngraph = builder.compile()\\n\\ninput_message = {\"role\": \"user\", \"content\": \"Hello\"}\\nresult = await graph.ainvoke({\"messages\": [input_message]})  \\n\\nAsync streaming\\nSee the streaming guide for examples of streaming with async.\\n\\u200bCombine control flow and state updates with Command\\nIt can be useful to combine control flow (edges) and state updates (nodes). For example, you might want to BOTH perform state updates AND decide which node to go to next in the SAME node. LangGraph provides a way to do so by returning a Command object from node functions:\\nCopydef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\\n    return Command(\\n        # state update\\n        update={\"foo\": \"bar\"},\\n        # control flow\\n        goto=\"my_other_node\"\\n    )\\n\\nWe show an end-to-end example below. Let‚Äôs create a simple graph with 3 nodes: A, B and C. We will first execute node A, and then decide whether to go to Node B or Node C next based on the output of node A.\\nCopyimport random\\nfrom typing_extensions import TypedDict, Literal\\nfrom langgraph.graph import StateGraph, START\\nfrom langgraph.types import Command\\n\\n# Define graph state\\nclass State(TypedDict):\\n    foo: str\\n\\n# Define the nodes\\n\\ndef node_a(state: State) -> Command[Literal[\"node_b\", \"node_c\"]]:\\n    print(\"Called A\")\\n    value = random.choice([\"b\", \"c\"])\\n    # this is a replacement for a conditional edge function\\n    if value == \"b\":\\n        goto = \"node_b\"\\n    else:\\n        goto = \"node_c\"\\n\\n    # note how Command allows you to BOTH update the graph state AND route to the next node\\n    return Command(\\n        # this is the state update\\n        update={\"foo\": value},\\n        # this is a replacement for an edge\\n        goto=goto,\\n    )\\n\\ndef node_b(state: State):\\n    print(\"Called B\")\\n    return {\"foo\": state[\"foo\"] + \"b\"}\\n\\ndef node_c(state: State):\\n    print(\"Called C\")\\n    return {\"foo\": state[\"foo\"] + \"c\"}\\n\\nWe can now create the StateGraph with the above nodes. Notice that the graph doesn‚Äôt have conditional edges for routing! This is because control flow is defined with Command inside node_a.\\nCopybuilder = StateGraph(State)\\nbuilder.add_edge(START, \"node_a\")\\nbuilder.add_node(node_a)\\nbuilder.add_node(node_b)\\nbuilder.add_node(node_c)\\n# NOTE: there are no edges between nodes A, B and C!\\n\\ngraph = builder.compile()\\n\\nYou might have noticed that we used Command as a return type annotation, e.g. Command[Literal[\"node_b\", \"node_c\"]]. This is necessary for the graph rendering and tells LangGraph that node_a can navigate to node_b and node_c.\\nCopyfrom IPython.display import display, Image\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nIf we run the graph multiple times, we‚Äôd see it take different paths (A -> B or A -> C) based on the random choice in node A.\\nCopygraph.invoke({\"foo\": \"\"})\\n\\nCopyCalled A\\nCalled C\\n\\n\\u200bNavigate to a node in a parent graph\\nIf you are using subgraphs, you might want to navigate from a node within a subgraph to a different subgraph (i.e. a different node in the parent graph). To do so, you can specify graph=Command.PARENT in Command:\\nCopydef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\\n    return Command(\\n        update={\"foo\": \"bar\"},\\n        goto=\"other_subgraph\",  # where `other_subgraph` is a node in the parent graph\\n        graph=Command.PARENT\\n    )\\n\\nLet‚Äôs demonstrate this using the above example. We‚Äôll do so by changing nodeA in the above example into a single-node graph that we‚Äôll add as a subgraph to our parent graph.\\nState updates with Command.PARENT\\nWhen you send updates from a subgraph node to a parent graph node for a key that‚Äôs shared by both parent and subgraph state schemas, you must define a reducer for the key you‚Äôre updating in the parent graph state. See the example below.\\nCopyimport operator\\nfrom typing_extensions import Annotated\\n\\nclass State(TypedDict):\\n    # NOTE: we define a reducer here\\n    foo: Annotated[str, operator.add]  \\n\\ndef node_a(state: State):\\n    print(\"Called A\")\\n    value = random.choice([\"a\", \"b\"])\\n    # this is a replacement for a conditional edge function\\n    if value == \"a\":\\n        goto = \"node_b\"\\n    else:\\n        goto = \"node_c\"\\n\\n    # note how Command allows you to BOTH update the graph state AND route to the next node\\n    return Command(\\n        update={\"foo\": value},\\n        goto=goto,\\n        # this tells LangGraph to navigate to node_b or node_c in the parent graph\\n        # NOTE: this will navigate to the closest parent graph relative to the subgraph\\n        graph=Command.PARENT,  \\n    )\\n\\nsubgraph = StateGraph(State).add_node(node_a).add_edge(START, \"node_a\").compile()\\n\\ndef node_b(state: State):\\n    print(\"Called B\")\\n    # NOTE: since we\\'ve defined a reducer, we don\\'t need to manually append\\n    # new characters to existing \\'foo\\' value. instead, reducer will append these\\n    # automatically (via operator.add)\\n    return {\"foo\": \"b\"}  \\n\\ndef node_c(state: State):\\n    print(\"Called C\")\\n    return {\"foo\": \"c\"}  \\n\\nbuilder = StateGraph(State)\\nbuilder.add_edge(START, \"subgraph\")\\nbuilder.add_node(\"subgraph\", subgraph)\\nbuilder.add_node(node_b)\\nbuilder.add_node(node_c)\\n\\ngraph = builder.compile()\\n\\nCopygraph.invoke({\"foo\": \"\"})\\n\\nCopyCalled A\\nCalled C\\n\\n\\u200bUse inside tools\\nA common use case is updating graph state from inside a tool. For example, in a customer support application you might want to look up customer information based on their account number or ID in the beginning of the conversation. To update the graph state from the tool, you can return Command(update={\"my_custom_key\": \"foo\", \"messages\": [...]}) from the tool:\\nCopy@tool\\ndef lookup_user_info(tool_call_id: Annotated[str, InjectedToolCallId], config: RunnableConfig):\\n    \"\"\"Use this to look up user information to better assist them with their questions.\"\"\"\\n    user_info = get_user_info(config.get(\"configurable\", {}).get(\"user_id\"))\\n    return Command(\\n        update={\\n            # update the state keys\\n            \"user_info\": user_info,\\n            # update the message history\\n            \"messages\": [ToolMessage(\"Successfully looked up user information\", tool_call_id=tool_call_id)]\\n        }\\n    )\\n\\nYou MUST include messages (or any state key used for the message history) in Command.update when returning Command from a tool and the list of messages in messages MUST contain a ToolMessage. This is necessary for the resulting message history to be valid (LLM providers require AI messages with tool calls to be followed by the tool result messages).\\nIf you are using tools that update state via Command, we recommend using prebuilt ToolNode which automatically handles tools returning Command objects and propagates them to the graph state. If you‚Äôre writing a custom node that calls tools, you would need to manually propagate Command objects returned by the tools as the update from the node.\\n\\u200bVisualize your graph\\nHere we demonstrate how to visualize the graphs you create.\\nYou can visualize any arbitrary Graph, including StateGraph.\\nLet‚Äôs have some fun by drawing fractals :).\\nCopyimport random\\nfrom typing import Annotated, Literal\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.graph.message import add_messages\\n\\nclass State(TypedDict):\\n    messages: Annotated[list, add_messages]\\n\\nclass MyNode:\\n    def __init__(self, name: str):\\n        self.name = name\\n    def __call__(self, state: State):\\n        return {\"messages\": [(\"assistant\", f\"Called node {self.name}\")]}\\n\\ndef route(state) -> Literal[\"entry_node\", END]:\\n    if len(state[\"messages\"]) > 10:\\n        return END\\n    return \"entry_node\"\\n\\ndef add_fractal_nodes(builder, current_node, level, max_level):\\n    if level > max_level:\\n        return\\n    # Number of nodes to create at this level\\n    num_nodes = random.randint(1, 3)  # Adjust randomness as needed\\n    for i in range(num_nodes):\\n        nm = [\"A\", \"B\", \"C\"][i]\\n        node_name = f\"node_{current_node}_{nm}\"\\n        builder.add_node(node_name, MyNode(node_name))\\n        builder.add_edge(current_node, node_name)\\n        # Recursively add more nodes\\n        r = random.random()\\n        if r > 0.2 and level + 1 < max_level:\\n            add_fractal_nodes(builder, node_name, level + 1, max_level)\\n        elif r > 0.05:\\n            builder.add_conditional_edges(node_name, route, node_name)\\n        else:\\n            # End\\n            builder.add_edge(node_name, END)\\n\\ndef build_fractal_graph(max_level: int):\\n    builder = StateGraph(State)\\n    entry_point = \"entry_node\"\\n    builder.add_node(entry_point, MyNode(entry_point))\\n    builder.add_edge(START, entry_point)\\n    add_fractal_nodes(builder, entry_point, 1, max_level)\\n    # Optional: set a finish point if required\\n    builder.add_edge(entry_point, END)  # or any specific node\\n    return builder.compile()\\n\\napp = build_fractal_graph(3)\\n\\n\\u200bMermaid\\nWe can also convert a graph class into Mermaid syntax.\\nCopyprint(app.get_graph().draw_mermaid())\\n\\nCopy%%{init: {\\'flowchart\\': {\\'curve\\': \\'linear\\'}}}%%\\ngraph TD;\\n    tart__([<p>__start__</p>]):::first\\n    ry_node(entry_node)\\n    e_entry_node_A(node_entry_node_A)\\n    e_entry_node_B(node_entry_node_B)\\n    e_node_entry_node_B_A(node_node_entry_node_B_A)\\n    e_node_entry_node_B_B(node_node_entry_node_B_B)\\n    e_node_entry_node_B_C(node_node_entry_node_B_C)\\n    nd__([<p>__end__</p>]):::last\\n    tart__ --> entry_node;\\n    ry_node --> __end__;\\n    ry_node --> node_entry_node_A;\\n    ry_node --> node_entry_node_B;\\n    e_entry_node_B --> node_node_entry_node_B_A;\\n    e_entry_node_B --> node_node_entry_node_B_B;\\n    e_entry_node_B --> node_node_entry_node_B_C;\\n    e_entry_node_A -.-> entry_node;\\n    e_entry_node_A -.-> __end__;\\n    e_node_entry_node_B_A -.-> entry_node;\\n    e_node_entry_node_B_A -.-> __end__;\\n    e_node_entry_node_B_B -.-> entry_node;\\n    e_node_entry_node_B_B -.-> __end__;\\n    e_node_entry_node_B_C -.-> entry_node;\\n    e_node_entry_node_B_C -.-> __end__;\\n    ssDef default fill:#f2f0ff,line-height:1.2\\n    ssDef first fill-opacity:0\\n    ssDef last fill:#bfb6fc\\n\\n\\u200bPNG\\nIf preferred, we could render the Graph into a .png. Here we could use three options:\\n\\nUsing Mermaid.ink API (does not require additional packages)\\nUsing Mermaid + Pyppeteer (requires pip install pyppeteer)\\nUsing graphviz (which requires pip install graphviz)\\n\\nUsing Mermaid.Ink\\nBy default, draw_mermaid_png() uses Mermaid.Ink‚Äôs API to generate the diagram.\\nCopyfrom IPython.display import Image, display\\nfrom langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\\n\\ndisplay(Image(app.get_graph().draw_mermaid_png()))\\n\\n\\nUsing Mermaid + Pyppeteer\\nCopyimport nest_asyncio\\n\\nnest_asyncio.apply()  # Required for Jupyter Notebook to run async functions\\n\\ndisplay(\\n    Image(\\n        app.get_graph().draw_mermaid_png(\\n            curve_style=CurveStyle.LINEAR,\\n            node_colors=NodeStyles(first=\"#ffdfba\", last=\"#baffc9\", default=\"#fad7de\"),\\n            wrap_label_n_words=9,\\n            output_file_path=None,\\n            draw_method=MermaidDrawMethod.PYPPETEER,\\n            background_color=\"white\",\\n            padding=10,\\n        )\\n    )\\n)\\n\\nUsing Graphviz\\nCopytry:\\n    display(Image(app.get_graph().draw_png()))\\nexcept ImportError:\\n    print(\\n        \"You likely need to install dependencies for pygraphviz, see more here https://github.com/pygraphviz/pygraphviz/blob/main/INSTALL.txt\"\\n    )\\n\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoGraph API overviewPreviousFunctional API overviewNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Load and extract docs from LangGraph docs\n",
    "\n",
    "urls_lg = [\n",
    "    \"https://docs.langchain.com/oss/python/langgraph/quickstart\",\n",
    "    \"https://docs.langchain.com/oss/python/langgraph/workflows-agents\",\n",
    "    \"https://docs.langchain.com/oss/python/langgraph/use-graph-api#map-reduce-and-the-send-api\"\n",
    "]\n",
    "\n",
    "# load the docs from the urls \n",
    "docs_lg = [WebBaseLoader(url).load() for url in urls_lg]\n",
    "docs_lg\n",
    "\n",
    "# extract the inner list from docs and flatten it \n",
    "flat_doc_list_lg = [doc for sublist in docs_lg for doc in sublist]\n",
    "flat_doc_list_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78854ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/overview', 'title': 'LangChain overview - Docs by LangChain', 'language': 'en'}, page_content='LangChain overview - Docs by LangChainSkip to main contentüöÄ Share how you\\'re building agents for a chance to win LangChain swag!Docs by LangChain home pageLangChain + LangGraphSearch...‚åòKAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChain overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingStructured outputMiddlewareOverviewBuilt-in middlewareCustom middlewareAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryAgent developmentLangSmith StudioTestAgent Chat UIDeploy with LangSmithDeploymentObservabilityOn this page Create an agent Core benefitsLangChain overviewCopy pageCopy pageLangChain v1.x is now available!For a complete list of changes and instructions on how to upgrade your code, see the release notes and migration guide.If you encounter any issues or have feedback, please open an issue so we can improve. To view v0.x documentation, go to the archived content.\\nLangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and more. LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use LangGraph, our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\\nLangChain agents are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\\n\\u200b Create an agent\\nCopy# pip install -qU langchain \"langchain[anthropic]\"\\nfrom langchain.agents import create_agent\\n\\ndef get_weather(city: str) -> str:\\n    \"\"\"Get weather for a given city.\"\"\"\\n    return f\"It\\'s always sunny in {city}!\"\\n\\nagent = create_agent(\\n    model=\"claude-sonnet-4-5-20250929\",\\n    tools=[get_weather],\\n    system_prompt=\"You are a helpful assistant\",\\n)\\n\\n# Run the agent\\nagent.invoke(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\\n)\\n\\nSee the Installation instructions and Quickstart guide to get started building your own agents and applications with LangChain.\\n\\u200b Core benefits\\nStandard model interfaceDifferent providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.Learn moreEasy to use, highly flexible agentLangChain‚Äôs agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.Learn moreBuilt on top of LangGraphLangChain‚Äôs agents are built on top of LangGraph. This allows us to take advantage of LangGraph‚Äôs durable execution, human-in-the-loop support, persistence, and more.Learn moreDebug with LangSmithGain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.Learn more\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoInstall LangChainNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='Agents - Docs by LangChainSkip to main contentüöÄ Share how you\\'re building agents for a chance to win LangChain swag!Docs by LangChain home pageLangChain + LangGraphSearch...‚åòKAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationCore componentsAgentsLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingStructured outputMiddlewareOverviewBuilt-in middlewareCustom middlewareAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryAgent developmentLangSmith StudioTestAgent Chat UIDeploy with LangSmithDeploymentObservabilityOn this pageCore componentsModelStatic modelDynamic modelToolsDefining toolsTool error handlingTool use in the ReAct loopSystem promptDynamic system promptInvocationAdvanced conceptsStructured outputToolStrategyProviderStrategyMemoryDefining state via middlewareDefining state via state_schemaStreamingMiddlewareCore componentsAgentsCopy pageCopy pageAgents combine language models with tools to create systems that can reason about tasks, decide which tools to use, and iteratively work towards solutions.\\ncreate_agent provides a production-ready agent implementation.\\nAn LLM Agent runs tools in a loop to achieve a goal.\\nAn agent runs until a stop condition is met - i.e., when the model emits a final output or an iteration limit is reached.\\n\\ncreate_agent builds a graph-based agent runtime using LangGraph. A graph consists of nodes (steps) and edges (connections) that define how your agent processes information. The agent moves through this graph, executing nodes like the model node (which calls the model), the tools node (which executes tools), or middleware.Learn more about the Graph API.\\n\\u200bCore components\\n\\u200bModel\\nThe model is the reasoning engine of your agent. It can be specified in multiple ways, supporting both static and dynamic model selection.\\n\\u200bStatic model\\nStatic models are configured once when creating the agent and remain unchanged throughout execution. This is the most common and straightforward approach.\\nTo initialize a static model from a model identifier string:\\nCopyfrom langchain.agents import create_agent\\n\\nagent = create_agent(\\n    \"gpt-5\",\\n    tools=tools\\n)\\n\\nModel identifier strings support automatic inference (e.g., \"gpt-5\" will be inferred as \"openai:gpt-5\"). Refer to the reference to see a full list of model identifier string mappings.\\nFor more control over the model configuration, initialize a model instance directly using the provider package. In this example, we use ChatOpenAI. See Chat models for other available chat model classes.\\nCopyfrom langchain.agents import create_agent\\nfrom langchain_openai import ChatOpenAI\\n\\nmodel = ChatOpenAI(\\n    model=\"gpt-5\",\\n    temperature=0.1,\\n    max_tokens=1000,\\n    timeout=30\\n    # ... (other params)\\n)\\nagent = create_agent(model, tools=tools)\\n\\nModel instances give you complete control over configuration. Use them when you need to set specific parameters like temperature, max_tokens, timeouts, base_url, and other provider-specific settings. Refer to the reference to see available params and methods on your model.\\n\\u200bDynamic model\\nDynamic models are selected at runtime based on the current state and context. This enables sophisticated routing logic and cost optimization.\\nTo use a dynamic model, create middleware using the @wrap_model_call decorator that modifies the model in the request:\\nCopyfrom langchain_openai import ChatOpenAI\\nfrom langchain.agents import create_agent\\nfrom langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\\n\\n\\nbasic_model = ChatOpenAI(model=\"gpt-4o-mini\")\\nadvanced_model = ChatOpenAI(model=\"gpt-4o\")\\n\\n@wrap_model_call\\ndef dynamic_model_selection(request: ModelRequest, handler) -> ModelResponse:\\n    \"\"\"Choose model based on conversation complexity.\"\"\"\\n    message_count = len(request.state[\"messages\"])\\n\\n    if message_count > 10:\\n        # Use an advanced model for longer conversations\\n        model = advanced_model\\n    else:\\n        model = basic_model\\n\\n    return handler(request.override(model=model))\\n\\nagent = create_agent(\\n    model=basic_model,  # Default model\\n    tools=tools,\\n    middleware=[dynamic_model_selection]\\n)\\n\\nPre-bound models (models with bind_tools already called) are not supported when using structured output. If you need dynamic model selection with structured output, ensure the models passed to the middleware are not pre-bound.\\nFor model configuration details, see Models. For dynamic model selection patterns, see Dynamic model in middleware.\\n\\u200bTools\\nTools give agents the ability to take actions. Agents go beyond simple model-only tool binding by facilitating:\\n\\nMultiple tool calls in sequence (triggered by a single prompt)\\nParallel tool calls when appropriate\\nDynamic tool selection based on previous results\\nTool retry logic and error handling\\nState persistence across tool calls\\n\\nFor more information, see Tools.\\n\\u200bDefining tools\\nPass a list of tools to the agent.\\nTools can be specified as plain Python functions or coroutines.The tool decorator can be used to customize tool names, descriptions, argument schemas, and other properties.\\nCopyfrom langchain.tools import tool\\nfrom langchain.agents import create_agent\\n\\n\\n@tool\\ndef search(query: str) -> str:\\n    \"\"\"Search for information.\"\"\"\\n    return f\"Results for: {query}\"\\n\\n@tool\\ndef get_weather(location: str) -> str:\\n    \"\"\"Get weather information for a location.\"\"\"\\n    return f\"Weather in {location}: Sunny, 72¬∞F\"\\n\\nagent = create_agent(model, tools=[search, get_weather])\\n\\nIf an empty tool list is provided, the agent will consist of a single LLM node without tool-calling capabilities.\\n\\u200bTool error handling\\nTo customize how tool errors are handled, use the @wrap_tool_call decorator to create middleware:\\nCopyfrom langchain.agents import create_agent\\nfrom langchain.agents.middleware import wrap_tool_call\\nfrom langchain.messages import ToolMessage\\n\\n\\n@wrap_tool_call\\ndef handle_tool_errors(request, handler):\\n    \"\"\"Handle tool execution errors with custom messages.\"\"\"\\n    try:\\n        return handler(request)\\n    except Exception as e:\\n        # Return a custom error message to the model\\n        return ToolMessage(\\n            content=f\"Tool error: Please check your input and try again. ({str(e)})\",\\n            tool_call_id=request.tool_call[\"id\"]\\n        )\\n\\nagent = create_agent(\\n    model=\"gpt-4o\",\\n    tools=[search, get_weather],\\n    middleware=[handle_tool_errors]\\n)\\n\\nThe agent will return a ToolMessage with the custom error message when a tool fails:\\nCopy[\\n    ...\\n    ToolMessage(\\n        content=\"Tool error: Please check your input and try again. (division by zero)\",\\n        tool_call_id=\"...\"\\n    ),\\n    ...\\n]\\n\\n\\u200bTool use in the ReAct loop\\nAgents follow the ReAct (‚ÄúReasoning + Acting‚Äù) pattern, alternating between brief reasoning steps with targeted tool calls and feeding the resulting observations into subsequent decisions until they can deliver a final answer.\\nExample of ReAct loopPrompt: Identify the current most popular wireless headphones and verify availability.Copy================================ Human Message =================================\\n\\nFind the most popular wireless headphones right now and check if they\\'re in stock\\n\\nReasoning: ‚ÄúPopularity is time-sensitive, I need to use the provided search tool.‚Äù\\nActing: Call search_products(\"wireless headphones\")\\nCopy================================== Ai Message ==================================\\nTool Calls:\\n  search_products (call_abc123)\\n Call ID: call_abc123\\n  Args:\\n    query: wireless headphones\\nCopy================================= Tool Message =================================\\n\\nFound 5 products matching \"wireless headphones\". Top 5 results: WH-1000XM5, ...\\n\\nReasoning: ‚ÄúI need to confirm availability for the top-ranked item before answering.‚Äù\\nActing: Call check_inventory(\"WH-1000XM5\")\\nCopy================================== Ai Message ==================================\\nTool Calls:\\n  check_inventory (call_def456)\\n Call ID: call_def456\\n  Args:\\n    product_id: WH-1000XM5\\nCopy================================= Tool Message =================================\\n\\nProduct WH-1000XM5: 10 units in stock\\n\\nReasoning: ‚ÄúI have the most popular model and its stock status. I can now answer the user‚Äôs question.‚Äù\\nActing: Produce final answer\\nCopy================================== Ai Message ==================================\\n\\nI found wireless headphones (model WH-1000XM5) with 10 units in stock...\\n\\nTo learn more about tools, see Tools.\\n\\u200bSystem prompt\\nYou can shape how your agent approaches tasks by providing a prompt. The system_prompt parameter can be provided as a string:\\nCopyagent = create_agent(\\n    model,\\n    tools,\\n    system_prompt=\"You are a helpful assistant. Be concise and accurate.\"\\n)\\n\\nWhen no system_prompt is provided, the agent will infer its task from the messages directly.\\n\\u200bDynamic system prompt\\nFor more advanced use cases where you need to modify the system prompt based on runtime context or agent state, you can use middleware.\\nThe @dynamic_prompt decorator creates middleware that generates system prompts based on the model request:\\nCopyfrom typing import TypedDict\\n\\nfrom langchain.agents import create_agent\\nfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\\n\\n\\nclass Context(TypedDict):\\n    user_role: str\\n\\n@dynamic_prompt\\ndef user_role_prompt(request: ModelRequest) -> str:\\n    \"\"\"Generate system prompt based on user role.\"\"\"\\n    user_role = request.runtime.context.get(\"user_role\", \"user\")\\n    base_prompt = \"You are a helpful assistant.\"\\n\\n    if user_role == \"expert\":\\n        return f\"{base_prompt} Provide detailed technical responses.\"\\n    elif user_role == \"beginner\":\\n        return f\"{base_prompt} Explain concepts simply and avoid jargon.\"\\n\\n    return base_prompt\\n\\nagent = create_agent(\\n    model=\"gpt-4o\",\\n    tools=[web_search],\\n    middleware=[user_role_prompt],\\n    context_schema=Context\\n)\\n\\n# The system prompt will be set dynamically based on context\\nresult = agent.invoke(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Explain machine learning\"}]},\\n    context={\"user_role\": \"expert\"}\\n)\\n\\nFor more details on message types and formatting, see Messages. For comprehensive middleware documentation, see Middleware.\\n\\u200bInvocation\\nYou can invoke an agent by passing an update to its State. All agents include a sequence of messages in their state; to invoke the agent, pass a new message:\\nCopyresult = agent.invoke(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What\\'s the weather in San Francisco?\"}]}\\n)\\n\\nFor streaming steps and / or tokens from the agent, refer to the streaming guide.\\nOtherwise, the agent follows the LangGraph Graph API and supports all associated methods, such as stream and invoke.\\n\\u200bAdvanced concepts\\n\\u200bStructured output\\nIn some situations, you may want the agent to return an output in a specific format. LangChain provides strategies for structured output via the response_format parameter.\\n\\u200bToolStrategy\\nToolStrategy uses artificial tool calling to generate structured output. This works with any model that supports tool calling:\\nCopyfrom pydantic import BaseModel\\nfrom langchain.agents import create_agent\\nfrom langchain.agents.structured_output import ToolStrategy\\n\\n\\nclass ContactInfo(BaseModel):\\n    name: str\\n    email: str\\n    phone: str\\n\\nagent = create_agent(\\n    model=\"gpt-4o-mini\",\\n    tools=[search_tool],\\n    response_format=ToolStrategy(ContactInfo)\\n)\\n\\nresult = agent.invoke({\\n    \"messages\": [{\"role\": \"user\", \"content\": \"Extract contact info from: John Doe, [email\\xa0protected], (555) 123-4567\"}]\\n})\\n\\nresult[\"structured_response\"]\\n# ContactInfo(name=\\'John Doe\\', email=\\'[email\\xa0protected]\\', phone=\\'(555) 123-4567\\')\\n\\n\\u200bProviderStrategy\\nProviderStrategy uses the model provider‚Äôs native structured output generation. This is more reliable but only works with providers that support native structured output (e.g., OpenAI):\\nCopyfrom langchain.agents.structured_output import ProviderStrategy\\n\\nagent = create_agent(\\n    model=\"gpt-4o\",\\n    response_format=ProviderStrategy(ContactInfo)\\n)\\n\\nAs of langchain 1.0, simply passing a schema (e.g., response_format=ContactInfo) is no longer supported. You must explicitly use ToolStrategy or ProviderStrategy.\\nTo learn about structured output, see Structured output.\\n\\u200bMemory\\nAgents maintain conversation history automatically through the message state. You can also configure the agent to use a custom state schema to remember additional information during the conversation.\\nInformation stored in the state can be thought of as the short-term memory of the agent:\\nCustom state schemas must extend AgentState as a TypedDict.\\nThere are two ways to define custom state:\\n\\nVia middleware (preferred)\\nVia state_schema on create_agent\\n\\n\\u200bDefining state via middleware\\nUse middleware to define custom state when your custom state needs to be accessed by specific middleware hooks and tools attached to said middleware.\\nCopyfrom langchain.agents import AgentState\\nfrom langchain.agents.middleware import AgentMiddleware\\nfrom typing import Any\\n\\n\\nclass CustomState(AgentState):\\n    user_preferences: dict\\n\\nclass CustomMiddleware(AgentMiddleware):\\n    state_schema = CustomState\\n    tools = [tool1, tool2]\\n\\n    def before_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\\n        ...\\n\\nagent = create_agent(\\n    model,\\n    tools=tools,\\n    middleware=[CustomMiddleware()]\\n)\\n\\n# The agent can now track additional state beyond messages\\nresult = agent.invoke({\\n    \"messages\": [{\"role\": \"user\", \"content\": \"I prefer technical explanations\"}],\\n    \"user_preferences\": {\"style\": \"technical\", \"verbosity\": \"detailed\"},\\n})\\n\\n\\u200bDefining state via state_schema\\nUse the state_schema parameter as a shortcut to define custom state that is only used in tools.\\nCopyfrom langchain.agents import AgentState\\n\\n\\nclass CustomState(AgentState):\\n    user_preferences: dict\\n\\nagent = create_agent(\\n    model,\\n    tools=[tool1, tool2],\\n    state_schema=CustomState\\n)\\n# The agent can now track additional state beyond messages\\nresult = agent.invoke({\\n    \"messages\": [{\"role\": \"user\", \"content\": \"I prefer technical explanations\"}],\\n    \"user_preferences\": {\"style\": \"technical\", \"verbosity\": \"detailed\"},\\n})\\n\\nAs of langchain 1.0, custom state schemas must be TypedDict types. Pydantic models and dataclasses are no longer supported. See the v1 migration guide for more details.\\nDefining custom state via middleware is preferred over defining it via state_schema on create_agent because it allows you to keep state extensions conceptually scoped to the relevant middleware and tools.state_schema is still supported for backwards compatibility on create_agent.\\nTo learn more about memory, see Memory. For information on implementing long-term memory that persists across sessions, see Long-term memory.\\n\\u200bStreaming\\nWe‚Äôve seen how the agent can be called with invoke to get a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\\nCopyfor chunk in agent.stream({\\n    \"messages\": [{\"role\": \"user\", \"content\": \"Search for AI news and summarize the findings\"}]\\n}, stream_mode=\"values\"):\\n    # Each chunk contains the full state at that point\\n    latest_message = chunk[\"messages\"][-1]\\n    if latest_message.content:\\n        print(f\"Agent: {latest_message.content}\")\\n    elif latest_message.tool_calls:\\n        print(f\"Calling tools: {[tc[\\'name\\'] for tc in latest_message.tool_calls]}\")\\n\\nFor more details on streaming, see Streaming.\\n\\u200bMiddleware\\nMiddleware provides powerful extensibility for customizing agent behavior at different stages of execution. You can use middleware to:\\n\\nProcess state before the model is called (e.g., message trimming, context injection)\\nModify or validate the model‚Äôs response (e.g., guardrails, content filtering)\\nHandle tool execution errors with custom logic\\nImplement dynamic model selection based on state or context\\nAdd custom logging, monitoring, or analytics\\n\\nMiddleware integrates seamlessly into the agent‚Äôs execution, allowing you to intercept and modify data flow at key points without changing the core agent logic.\\nFor comprehensive middleware documentation including decorators like @before_model, @after_model, and @wrap_tool_call, see Middleware.\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoPhilosophyPreviousModelsNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/guardrails', 'title': 'Guardrails - Docs by LangChain', 'description': 'Implement safety checks and content filtering for your agents', 'language': 'en'}, page_content='Guardrails - Docs by LangChainSkip to main contentüöÄ Share how you\\'re building agents for a chance to win LangChain swag!Docs by LangChain home pageLangChain + LangGraphSearch...‚åòKAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationAdvanced usageGuardrailsLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingStructured outputMiddlewareOverviewBuilt-in middlewareCustom middlewareAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryAgent developmentLangSmith StudioTestAgent Chat UIDeploy with LangSmithDeploymentObservabilityOn this pageBuilt-in guardrailsPII detectionHuman-in-the-loopCustom guardrailsBefore agent guardrailsAfter agent guardrailsCombine multiple guardrailsAdditional resourcesAdvanced usageGuardrailsCopy pageImplement safety checks and content filtering for your agentsCopy pageGuardrails help you build safe, compliant AI applications by validating and filtering content at key points in your agent‚Äôs execution. They can detect sensitive information, enforce content policies, validate outputs, and prevent unsafe behaviors before they cause problems.\\nCommon use cases include:\\n\\nPreventing PII leakage\\nDetecting and blocking prompt injection attacks\\nBlocking inappropriate or harmful content\\nEnforcing business rules and compliance requirements\\nValidating output quality and accuracy\\n\\nYou can implement guardrails using middleware to intercept execution at strategic points - before the agent starts, after it completes, or around model and tool calls.\\n\\nGuardrails can be implemented using two complementary approaches:\\nDeterministic guardrailsUse rule-based logic like regex patterns, keyword matching, or explicit checks. Fast, predictable, and cost-effective, but may miss nuanced violations.Model-based guardrailsUse LLMs or classifiers to evaluate content with semantic understanding. Catch subtle issues that rules miss, but are slower and more expensive.\\nLangChain provides both built-in guardrails (e.g., PII detection, human-in-the-loop) and a flexible middleware system for building custom guardrails using either approach.\\n\\u200bBuilt-in guardrails\\n\\u200bPII detection\\nLangChain provides built-in middleware for detecting and handling Personally Identifiable Information (PII) in conversations. This middleware can detect common PII types like emails, credit cards, IP addresses, and more.\\nPII detection middleware is helpful for cases such as health care and financial applications with compliance requirements, customer service agents that need to sanitize logs, and generally any application handling sensitive user data.\\nThe PII middleware supports multiple strategies for handling detected PII:\\nStrategyDescriptionExampleredactReplace with [REDACTED_TYPE][REDACTED_EMAIL]maskPartially obscure (e.g., last 4 digits)****-****-****-1234hashReplace with deterministic hasha8f5f167...blockRaise exception when detectedError thrown\\nCopyfrom langchain.agents import create_agent\\nfrom langchain.agents.middleware import PIIMiddleware\\n\\n\\nagent = create_agent(\\n    model=\"gpt-4o\",\\n    tools=[customer_service_tool, email_tool],\\n    middleware=[\\n        # Redact emails in user input before sending to model\\n        PIIMiddleware(\\n            \"email\",\\n            strategy=\"redact\",\\n            apply_to_input=True,\\n        ),\\n        # Mask credit cards in user input\\n        PIIMiddleware(\\n            \"credit_card\",\\n            strategy=\"mask\",\\n            apply_to_input=True,\\n        ),\\n        # Block API keys - raise error if detected\\n        PIIMiddleware(\\n            \"api_key\",\\n            detector=r\"sk-[a-zA-Z0-9]{32}\",\\n            strategy=\"block\",\\n            apply_to_input=True,\\n        ),\\n    ],\\n)\\n\\n# When user provides PII, it will be handled according to the strategy\\nresult = agent.invoke({\\n    \"messages\": [{\"role\": \"user\", \"content\": \"My email is [email\\xa0protected] and card is 5105-1051-0510-5100\"}]\\n})\\n\\nBuilt-in PII types and configurationBuilt-in PII types:\\nemail - Email addresses\\ncredit_card - Credit card numbers (Luhn validated)\\nip - IP addresses\\nmac_address - MAC addresses\\nurl - URLs\\nConfiguration options:ParameterDescriptionDefaultpii_typeType of PII to detect (built-in or custom)RequiredstrategyHow to handle detected PII (\"block\", \"redact\", \"mask\", \"hash\")\"redact\"detectorCustom detector function or regex patternNone (uses built-in)apply_to_inputCheck user messages before model callTrueapply_to_outputCheck AI messages after model callFalseapply_to_tool_resultsCheck tool result messages after executionFalse\\nSee the middleware documentation for complete details on PII detection capabilities.\\n\\u200bHuman-in-the-loop\\nLangChain provides built-in middleware for requiring human approval before executing sensitive operations. This is one of the most effective guardrails for high-stakes decisions.\\nHuman-in-the-loop middleware is helpful for cases such as financial transactions and transfers, deleting or modifying production data, sending communications to external parties, and any operation with significant business impact.\\nCopyfrom langchain.agents import create_agent\\nfrom langchain.agents.middleware import HumanInTheLoopMiddleware\\nfrom langgraph.checkpoint.memory import InMemorySaver\\nfrom langgraph.types import Command\\n\\n\\nagent = create_agent(\\n    model=\"gpt-4o\",\\n    tools=[search_tool, send_email_tool, delete_database_tool],\\n    middleware=[\\n        HumanInTheLoopMiddleware(\\n            interrupt_on={\\n                # Require approval for sensitive operations\\n                \"send_email\": True,\\n                \"delete_database\": True,\\n                # Auto-approve safe operations\\n                \"search\": False,\\n            }\\n        ),\\n    ],\\n    # Persist the state across interrupts\\n    checkpointer=InMemorySaver(),\\n)\\n\\n# Human-in-the-loop requires a thread ID for persistence\\nconfig = {\"configurable\": {\"thread_id\": \"some_id\"}}\\n\\n# Agent will pause and wait for approval before executing sensitive tools\\nresult = agent.invoke(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Send an email to the team\"}]},\\n    config=config\\n)\\n\\nresult = agent.invoke(\\n    Command(resume={\"decisions\": [{\"type\": \"approve\"}]}),\\n    config=config  # Same thread ID to resume the paused conversation\\n)\\n\\nSee the human-in-the-loop documentation for complete details on implementing approval workflows.\\n\\u200bCustom guardrails\\nFor more sophisticated guardrails, you can create custom middleware that runs before or after the agent executes. This gives you full control over validation logic, content filtering, and safety checks.\\n\\u200bBefore agent guardrails\\nUse ‚Äúbefore agent‚Äù hooks to validate requests once at the start of each invocation. This is useful for session-level checks like authentication, rate limiting, or blocking inappropriate requests before any processing begins.\\nClass syntaxDecorator syntaxCopyfrom typing import Any\\n\\nfrom langchain.agents.middleware import AgentMiddleware, AgentState, hook_config\\nfrom langgraph.runtime import Runtime\\n\\nclass ContentFilterMiddleware(AgentMiddleware):\\n    \"\"\"Deterministic guardrail: Block requests containing banned keywords.\"\"\"\\n\\n    def __init__(self, banned_keywords: list[str]):\\n        super().__init__()\\n        self.banned_keywords = [kw.lower() for kw in banned_keywords]\\n\\n    @hook_config(can_jump_to=[\"end\"])\\n    def before_agent(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\\n        # Get the first user message\\n        if not state[\"messages\"]:\\n            return None\\n\\n        first_message = state[\"messages\"][0]\\n        if first_message.type != \"human\":\\n            return None\\n\\n        content = first_message.content.lower()\\n\\n        # Check for banned keywords\\n        for keyword in self.banned_keywords:\\n            if keyword in content:\\n                # Block execution before any processing\\n                return {\\n                    \"messages\": [{\\n                        \"role\": \"assistant\",\\n                        \"content\": \"I cannot process requests containing inappropriate content. Please rephrase your request.\"\\n                    }],\\n                    \"jump_to\": \"end\"\\n                }\\n\\n        return None\\n\\n# Use the custom guardrail\\nfrom langchain.agents import create_agent\\n\\nagent = create_agent(\\n    model=\"gpt-4o\",\\n    tools=[search_tool, calculator_tool],\\n    middleware=[\\n        ContentFilterMiddleware(\\n            banned_keywords=[\"hack\", \"exploit\", \"malware\"]\\n        ),\\n    ],\\n)\\n\\n# This request will be blocked before any processing\\nresult = agent.invoke({\\n    \"messages\": [{\"role\": \"user\", \"content\": \"How do I hack into a database?\"}]\\n})\\n\\n\\u200bAfter agent guardrails\\nUse ‚Äúafter agent‚Äù hooks to validate final outputs once before returning to the user. This is useful for model-based safety checks, quality validation, or final compliance scans on the complete agent response.\\nClass syntaxDecorator syntaxCopyfrom langchain.agents.middleware import AgentMiddleware, AgentState, hook_config\\nfrom langgraph.runtime import Runtime\\nfrom langchain.messages import AIMessage\\nfrom langchain.chat_models import init_chat_model\\nfrom typing import Any\\n\\nclass SafetyGuardrailMiddleware(AgentMiddleware):\\n    \"\"\"Model-based guardrail: Use an LLM to evaluate response safety.\"\"\"\\n\\n    def __init__(self):\\n        super().__init__()\\n        self.safety_model = init_chat_model(\"gpt-4o-mini\")\\n\\n    @hook_config(can_jump_to=[\"end\"])\\n    def after_agent(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\\n        # Get the final AI response\\n        if not state[\"messages\"]:\\n            return None\\n\\n        last_message = state[\"messages\"][-1]\\n        if not isinstance(last_message, AIMessage):\\n            return None\\n\\n        # Use a model to evaluate safety\\n        safety_prompt = f\"\"\"Evaluate if this response is safe and appropriate.\\n        Respond with only \\'SAFE\\' or \\'UNSAFE\\'.\\n\\n        Response: {last_message.content}\"\"\"\\n\\n        result = self.safety_model.invoke([{\"role\": \"user\", \"content\": safety_prompt}])\\n\\n        if \"UNSAFE\" in result.content:\\n            last_message.content = \"I cannot provide that response. Please rephrase your request.\"\\n\\n        return None\\n\\n# Use the safety guardrail\\nfrom langchain.agents import create_agent\\n\\nagent = create_agent(\\n    model=\"gpt-4o\",\\n    tools=[search_tool, calculator_tool],\\n    middleware=[SafetyGuardrailMiddleware()],\\n)\\n\\nresult = agent.invoke({\\n    \"messages\": [{\"role\": \"user\", \"content\": \"How do I make explosives?\"}]\\n})\\n\\n\\u200bCombine multiple guardrails\\nYou can stack multiple guardrails by adding them to the middleware array. They execute in order, allowing you to build layered protection:\\nCopyfrom langchain.agents import create_agent\\nfrom langchain.agents.middleware import PIIMiddleware, HumanInTheLoopMiddleware\\n\\nagent = create_agent(\\n    model=\"gpt-4o\",\\n    tools=[search_tool, send_email_tool],\\n    middleware=[\\n        # Layer 1: Deterministic input filter (before agent)\\n        ContentFilterMiddleware(banned_keywords=[\"hack\", \"exploit\"]),\\n\\n        # Layer 2: PII protection (before and after model)\\n        PIIMiddleware(\"email\", strategy=\"redact\", apply_to_input=True),\\n        PIIMiddleware(\"email\", strategy=\"redact\", apply_to_output=True),\\n\\n        # Layer 3: Human approval for sensitive tools\\n        HumanInTheLoopMiddleware(interrupt_on={\"send_email\": True}),\\n\\n        # Layer 4: Model-based safety check (after agent)\\n        SafetyGuardrailMiddleware(),\\n    ],\\n)\\n\\n\\u200bAdditional resources\\n\\nMiddleware documentation - Complete guide to custom middleware\\nMiddleware API reference - Complete guide to custom middleware\\nHuman-in-the-loop - Add human review for sensitive operations\\nTesting agents - Strategies for testing safety mechanisms\\n\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoCustom middlewarePreviousRuntimeNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Load and extract docs from LangGraph docs\n",
    "\n",
    "urls_lc = [\n",
    "    \"https://docs.langchain.com/oss/python/langchain/overview\",\n",
    "    \"https://docs.langchain.com/oss/python/langchain/agents\",\n",
    "    \"https://docs.langchain.com/oss/python/langchain/guardrails\"\n",
    "]\n",
    "\n",
    "# load the docs from the urls \n",
    "docs_lc = [WebBaseLoader(url).load() for url in urls_lc]\n",
    "docs_lc\n",
    "\n",
    "# extract the inner list from docs and flatten it \n",
    "flat_docs_list_lc = [doc for sublist in docs_lc for doc in sublist]\n",
    "flat_docs_list_lc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2ed9b6",
   "metadata": {},
   "source": [
    "### Text Splitter, Document Store (Vector Store) and Retriever (Setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f182029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up text splitter - recursive character based with chunk size of 1000 and chunk overlap of 100\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a4fcc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='e86ebfc7-f5a1-4817-83f6-0c910d430d1f', metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Compile the agent\\nagent = agent_builder.compile()\\n\\n# Show the agent\\ndisplay(Image(agent.get_graph(xray=True).draw_mermaid_png()))\\n\\n# Invoke\\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\\nmessages = agent.invoke({\"messages\": messages})\\nfor m in messages[\"messages\"]:\\n    m.pretty_print()\\n\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoThinking in LangGraphPreviousPersistenceNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'), Document(id='c4c220f5-b8f2-4bc0-9e47-7cb92ab724e7', metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content=\"Workflows and agents - Docs by LangChainSkip to main contentüöÄ Share how you're building agents for a chance to win LangChain swag!Docs by LangChain home pageLangChain + LangGraphSearch...‚åòKAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationGet startedWorkflows and agentsLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureTestLangSmith StudioAgent Chat UILangSmith DeploymentLangSmith ObservabilityLangGraph APIsGraph APIFunctional APIRuntimeOn this pageSetupLLMs and augmentationsPrompt chainingParallelizationRoutingOrchestrator-workerCreating workers in LangGraphEvaluator-optimizerAgentsGet startedWorkflows and agentsCopy pageCopy pageThis guide reviews common workflow and agent patterns.\"), Document(id='60dd1cb4-56cc-483a-a315-6d9c19f06df4', metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/quickstart', 'title': 'Quickstart - Docs by LangChain', 'language': 'en'}, page_content='# Otherwise, we stop (reply to the user)\\n    return END\\n\\u200b6. Build and compile the agentThe agent is built using the StateGraph class and compiled using the compile method.Copy# Build workflow\\nagent_builder = StateGraph(MessagesState)\\n\\n# Add nodes\\nagent_builder.add_node(\"llm_call\", llm_call)\\nagent_builder.add_node(\"tool_node\", tool_node)\\n\\n# Add edges to connect nodes\\nagent_builder.add_edge(START, \"llm_call\")\\nagent_builder.add_conditional_edges(\\n    \"llm_call\",\\n    should_continue,\\n    [\"tool_node\", END]\\n)\\nagent_builder.add_edge(\"tool_node\", \"llm_call\")\\n\\n# Compile the agent\\nagent = agent_builder.compile()\\n\\n# Show the agent\\nfrom IPython.display import Image, display\\ndisplay(Image(agent.get_graph(xray=True).draw_mermaid_png()))'), Document(id='46efe80b-c2fe-41f4-ad44-a004de4fe745', metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='Workflows have predetermined code paths and are designed to operate in a certain order.\\nAgents are dynamic and define their own processes and tool usage.\\n\\n\\nLangGraph offers several benefits when building agents and workflows, including persistence, streaming, and support for debugging as well as deployment.\\n\\u200bSetup\\nTo build a workflow or agent, you can use any chat model that supports structured outputs and tool calling. The following example uses Anthropic:\\n\\nInstall dependencies:\\n\\nCopypip install langchain_core langchain-anthropic langgraph\\n\\n\\nInitialize the LLM:\\n\\nCopyimport os\\nimport getpass\\n\\nfrom langchain_anthropic import ChatAnthropic\\n\\ndef _set_env(var: str):\\n    if not os.environ.get(var):\\n        os.environ[var] = getpass.getpass(f\"{var}: \")\\n\\n\\n_set_env(\"ANTHROPIC_API_KEY\")\\n\\nllm = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\")')]\n"
     ]
    }
   ],
   "source": [
    "doc_chunks_lg = text_splitter.split_documents(flat_doc_list_lg)\n",
    "doc_chunks_lg\n",
    "\n",
    "vectorstore_faiss = FAISS.from_documents(doc_chunks_lg, embeddings)\n",
    "retriever_lg = vectorstore_faiss.as_retriever()\n",
    "\n",
    "print(retriever_lg.invoke(\"How to build agents?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63770616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='b9a17390-505d-43b4-9432-e5bb5d897eda', metadata={'source': 'https://docs.langchain.com/oss/python/langchain/overview', 'title': 'LangChain overview - Docs by LangChain', 'language': 'en'}, page_content='LangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and more. LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use LangGraph, our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\\nLangChain agents are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\\n\\u200b Create an agent\\nCopy# pip install -qU langchain \"langchain[anthropic]\"\\nfrom langchain.agents import create_agent'), Document(id='d1d6f695-3043-4f4a-aeea-5b25f0c245ba', metadata={'language': 'en', 'title': 'LangChain overview - Docs by LangChain', 'source': 'https://docs.langchain.com/oss/python/langchain/overview'}, page_content='See the Installation instructions and Quickstart guide to get started building your own agents and applications with LangChain.\\n\\u200b Core benefits'), Document(id='33a54ec2-4313-4521-84e2-286e6c47505a', metadata={'language': 'en', 'source': 'https://docs.langchain.com/oss/python/langchain/overview', 'title': 'LangChain overview - Docs by LangChain'}, page_content='Edit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoInstall LangChainNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'), Document(id='a131ac29-9090-46b0-a12b-662fa18e2624', metadata={'language': 'en', 'title': 'Agents - Docs by LangChain', 'source': 'https://docs.langchain.com/oss/python/langchain/agents'}, page_content='Edit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoPhilosophyPreviousModelsNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')]\n"
     ]
    }
   ],
   "source": [
    "doc_lc_chunks = text_splitter.split_documents(flat_docs_list_lc)\n",
    "doc_lc_chunks\n",
    "\n",
    "vectorstore_chroma = Chroma.from_documents(doc_lc_chunks, embeddings)\n",
    "retriever_lc = vectorstore_chroma.as_retriever()\n",
    "\n",
    "print(retriever_lc.invoke(\"How to build chains?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ba2320",
   "metadata": {},
   "source": [
    "### Setup Retriever as a tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6086c762",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def retriever_lg_vector_db_blog(query: str) -> str:\n",
    "    \"\"\"Search and extract information about LangGraph\"\"\"\n",
    "    docs = retriever_lg.invoke(query)\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "@tool\n",
    "def retriever_lc_vector_db_blog(query: str) -> str:\n",
    "    \"\"\"Search and extract information about LangChain\"\"\"\n",
    "    docs = retriever_lc.invoke(query)\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "retriever_langgraph_tool = retriever_lg_vector_db_blog\n",
    "retriever_langchain_tool = retriever_lc_vector_db_blog\n",
    "\n",
    "tools = [retriever_langgraph_tool, retriever_langchain_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81c1b3c",
   "metadata": {},
   "source": [
    "### Status Till Now:\n",
    "\n",
    "- Extracted documents from web about LangGraph\n",
    "- It has information about LangGraph Implementation\n",
    "- Used FAISS vectorstore to save the document and chunking them\n",
    "- Created vectorstore as retriever\n",
    "- Created retriever tool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248297f6",
   "metadata": {},
   "source": [
    "### Combine tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d53eca47",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [retriever_langgraph_tool, retriever_langchain_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280df788",
   "metadata": {},
   "source": [
    "### Bind the Model with tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96ae37ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_tools = model.bind_tools(tools=tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cb2357",
   "metadata": {},
   "source": [
    "### Build Graph\n",
    "\n",
    "Step 1: Create an Agent that will use model with tools to get relavent documents based on query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c2af6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    # add_messages is a reducer function, that defines how to add messages to the list\n",
    "    # by default state would replace the entry, but with add_messages it would append to the list\n",
    "    messages: Annotated[list[AnyMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b185145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_node(state: AgentState) -> AgentState:\n",
    "    \"\"\" \n",
    "    Invokes the agent model to generate a response based on current state. Given the\n",
    "    question, it will decide to call the retriever tool or end the call\"\"\"\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    response = model_with_tools.invoke(messages)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83404a69",
   "metadata": {},
   "source": [
    "### Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a2090be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Structured Schema for Grading\n",
    "class GradeSchema(BaseModel):\n",
    "    \"\"\"Binary score for relevance check\"\"\"\n",
    "    binary_score: Literal[\"yes\", \"no\"] = Field(description=\"Binary score for relevance check, with yes or no\")\n",
    "\n",
    "# Now update the Model with this schema so that we can get only binary score\n",
    "model_with_schema = model.with_structured_output(GradeSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7947eb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_docs_node(state: AgentState) -> Literal[\"generate\", \"rewrite\"]:\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved docs are good enough to answer the question. \n",
    "    If yes, generate the answer.\n",
    "    If not, rewrite the question.\n",
    "    \"\"\"\n",
    "\n",
    "    template = \"\"\"\n",
    "    You are an assistant that will assess the relevance of retrieved docs to answer a question.\n",
    "\n",
    "    Here is the retrieved docs: \\n{retrieved_docs}\\n\n",
    "\n",
    "    Question: \\n{question}\\n\n",
    "\n",
    "    If the retrieved docs are relevant keyword(s) and symantic meaning related to the question asked, \n",
    "    then grade with 'yes'. Otherwise, answer with 'no'.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    chain = prompt | model_with_schema\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    question = messages[0].content\n",
    "    retrieved_docs = messages[-1].content\n",
    "\n",
    "    response = chain.invoke({\"retrieved_docs\": retrieved_docs, \"question\": question})\n",
    "    score = response.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        print(f\"Docs are good enough to answer the question, score: {score}, verdict: generate\")\n",
    "        return \"generate\"\n",
    "    else:\n",
    "        print(f\"Docs are not good enough to answer the question, score: {score}, verdict: rewrite\")\n",
    "        return \"rewrite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7eb89e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_node(state: AgentState) -> AgentState:\n",
    "\n",
    "    client = Client()\n",
    "    prompt = client.pull_prompt(\"rlm/rag-prompt\")\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    question = messages[0].content\n",
    "    retrieved_docs = messages[-1].content\n",
    "\n",
    "    parser = StrOutputParser()\n",
    "\n",
    "    rag_chain = prompt | model | parser\n",
    "\n",
    "    response = rag_chain.invoke({\"context\": retrieved_docs, \"question\": question})\n",
    "\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b39f002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_question_node(state: AgentState) -> AgentState:\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    question = messages[0].content\n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(content=f\"You are an assistant that will rewrite the question\"),\n",
    "        HumanMessage(content=f\"\"\"Look at the question below and rewrite it, \n",
    "                     to be more specific and clear based symantic intent \\n\\n \n",
    "                     Question: {question} \\n\\n\n",
    "                     Rewritten Question: \"\"\"),\n",
    "    ]\n",
    "\n",
    "    response = model.invoke(messages)\n",
    "\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5c5ddb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_node = ToolNode(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa719ad",
   "metadata": {},
   "source": [
    "### Create a State Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "692bcba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x15855e510>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = StateGraph(AgentState)\n",
    "\n",
    "graph.add_node(\"agent\", agent_node)\n",
    "graph.add_node(\"retriever\", retriever_node)\n",
    "graph.add_node(\"generate_answer\", generate_answer_node)\n",
    "graph.add_node(\"rewrite_question\", rewrite_question_node)\n",
    "\n",
    "graph.add_edge(START, \"agent\")\n",
    "graph.add_conditional_edges(\"agent\", tools_condition, {\"tools\": \"retriever\", END: END})\n",
    "graph.add_conditional_edges(\"retriever\", grade_docs_node, {\"generate\": \"generate_answer\", \"rewrite\": \"rewrite_question\"})\n",
    "graph.add_edge(\"rewrite_question\", \"agent\")\n",
    "graph.add_edge(\"generate_answer\", END)\n",
    "\n",
    "workflow = graph.compile()\n",
    "workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd4ddcdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs are good enough to answer the question, score: yes, verdict: generate\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is LangChain?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retriever_lc_vector_db_blog (7fryv4hkc)\n",
      " Call ID: 7fryv4hkc\n",
      "  Args:\n",
      "    query: What is LangChain?\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retriever_lc_vector_db_blog\n",
      "\n",
      "LangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and more. LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\n",
      "We recommend you use LangChain if you want to quickly build agents and autonomous applications. Use LangGraph, our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\n",
      "LangChain agents are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\n",
      "‚Äã Create an agent\n",
      "Copy# pip install -qU langchain \"langchain[anthropic]\"\n",
      "from langchain.agents import create_agent\n",
      "\n",
      "LangChain overview - Docs by LangChainSkip to main contentüöÄ Share how you're building agents for a chance to win LangChain swag!Docs by LangChain home pageLangChain + LangGraphSearch...‚åòKAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChain overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingStructured outputMiddlewareOverviewBuilt-in middlewareCustom middlewareAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryAgent developmentLangSmith StudioTestAgent Chat UIDeploy with LangSmithDeploymentObservabilityOn this page Create an agent Core benefitsLangChain overviewCopy pageCopy pageLangChain v1.x is now available!For a complete list of changes and instructions on how to upgrade your code, see the release notes and migration guide.If you encounter any issues or\n",
      "\n",
      "See the Installation instructions and Quickstart guide to get started building your own agents and applications with LangChain.\n",
      "‚Äã Core benefits\n",
      "\n",
      "‚Äã Core benefits\n",
      "Standard model interfaceDifferent providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.Learn moreEasy to use, highly flexible agentLangChain‚Äôs agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.Learn moreBuilt on top of LangGraphLangChain‚Äôs agents are built on top of LangGraph. This allows us to take advantage of LangGraph‚Äôs durable execution, human-in-the-loop support, persistence, and more.Learn moreDebug with LangSmithGain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.Learn more\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "<think>\n",
      "Okay, the user is asking what LangChain is. Let me check the provided context.\n",
      "\n",
      "The context starts by saying LangChain is the easiest way to build agents and apps using LLMs. It mentions connecting to various providers like OpenAI and Anthropic with minimal code. Also, it provides pre-built agent architecture and model integrations. Then there's a comparison with LangGraph, which is a lower-level framework. LangChain agents are built on LangGraph for features like durable execution and streaming. The core benefits include a standard model interface, ease of use, flexibility, and debugging with LangSmith.\n",
      "\n",
      "So, the answer should mention that LangChain is a framework for building agents and apps with LLMs, highlights ease of use with quick setup, integration with multiple providers, and built-on LangGraph for advanced features. Keep it concise, three sentences max. Avoid technical jargon but include key points like standardizing APIs, flexibility, and debugging tools. Make sure not to include any markdown and keep each sentence clear.\n",
      "</think>\n",
      "\n",
      "LangChain is a framework for building agents and applications powered by large language models (LLLMs), offering pre-built architectures and integrations with providers like OpenAI and Anthropic. It simplifies development with a standardized model interface, enabling seamless switching between providers and reducing API complexity. Built on LangGraph, it supports advanced features like durable execution, human-in-the-loop interactions, and debugging via LangSmith.\n"
     ]
    }
   ],
   "source": [
    "initial_state = {\"messages\": [HumanMessage(content=\"What is LangChain?\")]}\n",
    "\n",
    "output = workflow.invoke(initial_state)\n",
    "\n",
    "for message in output[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "938dcaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is Machine Learning?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The user is asking about Machine Learning in general, but the available tools are designed to search for information specifically related to LangGraph and LangChain. Since there is no function to retrieve general Machine Learning definitions, I cannot use the provided tools to answer this query. \n",
      "\n",
      "However, here's a brief explanation: **Machine Learning** is a subset of artificial intelligence that involves training algorithms to learn patterns from data, enabling them to make decisions or predictions without being explicitly programmed for each task. It includes techniques like supervised learning, unsupervised learning, and reinforcement learning.\n"
     ]
    }
   ],
   "source": [
    "initial_state = {\"messages\": [HumanMessage(content=\"What is Machine Learning?\")]}\n",
    "\n",
    "output = workflow.invoke(initial_state)\n",
    "\n",
    "for message in output[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8264f3f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agenticai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
