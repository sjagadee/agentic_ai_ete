{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7356584",
   "metadata": {},
   "source": [
    "### Build simple LLM using Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c69e886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "load_dotenv() ## loading all the environment variables from the .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93262d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of India is New Delhi.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 42, 'total_tokens': 51, 'completion_time': 0.006906185, 'completion_tokens_details': None, 'prompt_time': 0.002000321, 'prompt_tokens_details': None, 'queue_time': 0.00547224, 'total_time': 0.008906506}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_9ca2574dca', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--ff476af9-759b-4419-8a4c-b9d1ae569727-0', usage_metadata={'input_tokens': 42, 'output_tokens': 9, 'total_tokens': 51})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ChatGroq(model=\"llama-3.1-8b-instant\")\n",
    "model.invoke(\"what is the capital of india?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8070c66",
   "metadata": {},
   "source": [
    "Using messages we can interact with LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df539544",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "    HumanMessage(content=\"Hello, how are you?\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4ba9834",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.invoke(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa6ff1b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Bonjour, je vais bien, merci. Comment puis-je vous aider aujourd'hui ?\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with string output parser\n",
    "parser = StrOutputParser()\n",
    "parser.invoke(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7566e543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Bonjour, comment allez-vous ? \\n\\n(I'm doing well, thank you for asking. How about you?)\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using langchain expression language\n",
    "\n",
    "chain = model | parser\n",
    "chain.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6430feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template\n",
    "\n",
    "generic_template = \"Translate following into {language}\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", generic_template),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c68ec082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Translate following into French', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello, how are you?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = prompt.invoke({\"language\": \"French\", \"text\": \"Hello, how are you?\"})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4023af80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Translate following into French', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hello, how are you?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.to_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524fd48f",
   "metadata": {},
   "source": [
    "If you see the messages above is equivalent to the output of this above output, this is the power of chat prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e6df1034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bonjour, comment Ã§a va ?'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new let us create a full chain\n",
    "\n",
    "final_chain = prompt | model | parser\n",
    "final_chain.invoke({\"language\": \"French\", \"text\": \"Hello, how are you?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6228ebc0",
   "metadata": {},
   "source": [
    "Here let me explain step by step:\n",
    "- Prompt is being created using ChatPromptTemplate \n",
    "- Model we had created above using Groq's llama3.1\n",
    "- We have StrOutputParser\n",
    "\n",
    "We individually invoked them, we were able to get the output.\n",
    "\n",
    "Here the final_chain we created will work like this:\n",
    "- Take the input we have passed while calling .invoke({...})\n",
    "- First parser - will internally call parser.invoke({...}) - gives messages as output\n",
    "- These messages are passed as input to model, model.invoke(messages) internally gets called\n",
    "    - output from this step => AIMessage(content=\"....\")\n",
    "- This above output is passed to parser, parser internally calls parser.invoke(AIMessage(content=\"....\"))\n",
    "    - output is \"....\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df13030e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agenticai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
