{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6d3f0a0",
   "metadata": {},
   "source": [
    "### Lets build chatbots using langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "592d24ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "517cb067",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGroq(model=\"llama-3.1-8b-instant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d2f42c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Nice to meet you, Srini! I'm happy to chat with you. Is there something I can help you with, or would you like to just say hello and see where the conversation goes?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 43, 'total_tokens': 84, 'completion_time': 0.060703172, 'completion_tokens_details': None, 'prompt_time': 0.002079811, 'prompt_tokens_details': None, 'queue_time': 0.005433615, 'total_time': 0.062782983}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_9ca2574dca', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--1b4b56bf-fe9a-4ab0-98b7-803ce0e1164c-0', usage_metadata={'input_tokens': 43, 'output_tokens': 41, 'total_tokens': 84})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets invoke the model\n",
    "model.invoke([HumanMessage(content=\"Hi, my name is srini!\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf40fa16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I don't have any information about your name. I'm a large language model, I don't have the ability to retain personal information or recall previous conversations. Each time you interact with me, it's a new conversation and I don't have any prior knowledge about you. If you'd like to share your name with me, I'd be happy to chat with you!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 76, 'prompt_tokens': 40, 'total_tokens': 116, 'completion_time': 0.093150409, 'completion_tokens_details': None, 'prompt_time': 0.001846363, 'prompt_tokens_details': None, 'queue_time': 0.005630164, 'total_time': 0.094996772}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_9ca2574dca', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--8f40840e-046e-4f70-b889-2dae0c7f682c-0', usage_metadata={'input_tokens': 40, 'output_tokens': 76, 'total_tokens': 116})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(content=\"What is my name?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e26b8792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Srini.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 97, 'total_tokens': 104, 'completion_time': 0.009091895, 'completion_tokens_details': None, 'prompt_time': 0.005581137, 'prompt_tokens_details': None, 'queue_time': 0.005117414, 'total_time': 0.014673032}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_9ca2574dca', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--b358abe8-b46e-4914-95db-5be4c2e627e1-0', usage_metadata={'input_tokens': 97, 'output_tokens': 7, 'total_tokens': 104})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets invoke the model\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi, my name is srini!\"),\n",
    "        AIMessage(content=\"Nice to meet you, Srini! I'm happy to chat with you. Is there something on your mind that you'd like to talk about, or do you just want to say hello?\"),\n",
    "        HumanMessage(content=\"What is my name?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247801b7",
   "metadata": {},
   "source": [
    "As we see here:\n",
    "- If we do not provide the context. LLM will not have any information\n",
    "- If we provide the context, it can easily know and answer based on context\n",
    "\n",
    "This is called illusion of memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e9e8f2",
   "metadata": {},
   "source": [
    "### Message History\n",
    "\n",
    "We can use a Message History class to wrap our model and make it stateful. This will keep track of inputs and outputs of the model, and store them in some datastore. Future interactions will then load those messages and pass them into the chain as part of the input. Let's see how to use this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "047e2161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# history store\n",
    "store = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc1628bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to get the chat history based on session id\n",
    "\n",
    "def get_chat_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07568426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup config\n",
    "config = {'configurable': {'session_id': 'chat_1'}}\n",
    "\n",
    "# setup model with the session history\n",
    "with_chat_history = RunnableWithMessageHistory(model, get_session_history=get_chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1b84220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interact with model with session history\n",
    "response = with_chat_history.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi, my name is srini! I am Senior GenAI Engineer!\"),\n",
    "    ],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "872e8de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Srini, Senior GenAI Engineer! It's great to meet you. I'm a large language model, and I'm here to assist and learn from you. What brings you here today? Are you working on a new AI project, or do you need help with something specific?\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e8bcd86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm happy to chat with you, but I don't have any information about your name. I'm a large language model, I don't have the ability to retain information about individual users or their personal details. Each time you interact with me, it's a new conversation and I don't retain any context or information from previous conversations.\\n\\nIf you'd like to share your name with me, I'd be happy to chat with you and use it in our conversation. Or, if you'd prefer, we can keep our conversation anonymous and use pronouns or a username instead. Let me know what you prefer!\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change the config - new session id\n",
    "config1 = {'configurable': {'session_id': 'chat_2'}}\n",
    "\n",
    "# interact with model with session history\n",
    "response = with_chat_history.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi, What is my name?\"),\n",
    "    ],\n",
    "    config=config1\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34c15fcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Your name is Srini, and you're a Senior GenAI Engineer.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# interact with model with session history - with the first session\n",
    "response = with_chat_history.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi, What is my name?\"),\n",
    "    ],\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe992b5",
   "metadata": {},
   "source": [
    "### Prompt templates\n",
    "\n",
    "Prompt Templates help to turn raw user information into a format that the LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM. Let's now make that a bit more complicated. First, let's add in a system message with some custom instructions (but still taking messages as input). Next, we'll add in more input besides just the messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d52c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant. Answer all the question to the best of your ability.\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5716f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Srini, I'm glad you're here. How can I assist you today?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 60, 'total_tokens': 79, 'completion_time': 0.035467118, 'completion_tokens_details': None, 'prompt_time': 0.003287779, 'prompt_tokens_details': None, 'queue_time': 0.005474608, 'total_time': 0.038754897}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_9ca2574dca', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--e5428493-ee25-47bc-9cd6-1b32efdb5b32-0', usage_metadata={'input_tokens': 60, 'output_tokens': 19, 'total_tokens': 79})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we have messages placeholder setup to accept \"messages\"\n",
    "# so, when we invoke the chain we need to pass the messages to the chain\n",
    "\n",
    "chain.invoke({\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"Hi, My name is srini!\"),\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9de5a669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us set the chain with session history, by providing chain with the session history\n",
    "# chain = prompt | model\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(chain, get_session_history=get_chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "103dd195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Srini! It's nice to meet you. I'm here to help with any questions or topics you'd like to discuss. How's your day going so far?\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup new config \n",
    "config2 = {'configurable': {'session_id': 'chat_3'}}\n",
    "\n",
    "# interact with model with session history\n",
    "response = chain_with_history.invoke(\n",
    "    [HumanMessage(content=\"Hi, My name is srini!\")],\n",
    "    config=config2\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d971ed5c",
   "metadata": {},
   "source": [
    "When you call chain_with_history.invoke(), the RunnableWithMessageHistory wrapper:\n",
    "\n",
    "1. Takes your input messages: [HumanMessage(content=\"Hi, My name is srini!\")]\n",
    "2. Retrieves the chat history from the session (via get_chat_history)\n",
    "3. Combines them together\n",
    "4. Passes the combined messages to the underlying chain in the format it expects: {\"messages\": [history... + new messages...]}\n",
    "5. Saves the new messages to the history after getting the response\n",
    "\n",
    "So you are effectively sending messages - the RunnableWithMessageHistory wrapper is just doing the work of formatting them and managing the history for you\n",
    "automatically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf0bfee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding more complexity to the chain\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant. Answer all the question to the best of your ability in {language} Language.\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2fa39b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='नमस्ते Srini! मुझे खुशी है आपकी मुलाकात करने का मौका मिला है। मैं आपकी सहायता के लिए यहाँ हूँ। क्या आपके पास कोई प्रश्न या समस्या है जिससे मैं आपकी मदद कर सकता हूँ?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 84, 'prompt_tokens': 63, 'total_tokens': 147, 'completion_time': 0.112864252, 'completion_tokens_details': None, 'prompt_time': 0.003542867, 'prompt_tokens_details': None, 'queue_time': 0.005432938, 'total_time': 0.116407119}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_9ca2574dca', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--33461484-0718-4657-ae91-9f487b9e829a-0', usage_metadata={'input_tokens': 63, 'output_tokens': 84, 'total_tokens': 147})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first lets directly call chain\n",
    "chain.invoke({\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"Hi, My name is srini!\"),\n",
    "    ],\n",
    "    \"language\": \"Hindi\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3e70b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_with_history_v2 = RunnableWithMessageHistory(chain, get_session_history=get_chat_history, input_messages_key=\"messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6157fc66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'नमस्ते!  मैं आपका सहायक हूँ, और मैं आपकी सहायता के लिए तैयार हूँ। आपका नाम स्रीनि है (Srini), मैं आपके साथ बात करने के लिए उत्साहित हूँ। क्या आपके पास कोई प्रश्न है या क्या आप कुछ जानना चाहते हैं?'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config3 = {'configurable': {'session_id': 'chat_4'}}\n",
    "\n",
    "response = chain_with_history_v2.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"Hi, My name is srini!\"),\n",
    "        ],\n",
    "        \"language\": \"Hindi\",\n",
    "    },\n",
    "    config=config3\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835c409e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'आपका नाम स्रीनि (Srini) है ।'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain_with_history_v2.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"What is my name?\"),\n",
    "        ],\n",
    "        \"language\": \"Hindi\",\n",
    "    },\n",
    "    config=config3\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334784ab",
   "metadata": {},
   "source": [
    "In chain_with_history - which is a wrapper of RunnableWithMessageHistory (combining chain and session history) \n",
    "- assumes that the whole input is \"messages\" that is specified in ChatPromptTemplate's MessagesPlaceholder\n",
    "\n",
    "In chain_with_history - which is a wrapper of RunnableWithMessageHistory (combining chain and session history) \n",
    "- it also has `input_messages_key=\"messages\"`\n",
    "- this DOES NOT assumes that the whole input is \"messages\" that is specified in ChatPromptTemplate's MessagesPlaceholder\n",
    "- need to specify \"messages\" and pass all the required messages here such has HumanMessage etc\n",
    "- advantage is that we can pass other variables present in chat prompt template such as \"language\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca68450",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agenticai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
